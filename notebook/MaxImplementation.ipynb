{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time, random\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from pyspark import SparkContext, SparkConf, rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize values\n",
    "def normalize_data(data):\n",
    "    max_min = data.flatMap(lambda x: [ (index_key, x[index_key]) for index_key in range(len(x)-1)]) #Last position is label\n",
    "    max__list = sorted(max_min.reduceByKey(lambda x,y: x if x > y else y).collect())\n",
    "    min__list = sorted(max_min.reduceByKey(lambda x,y: x if x < y else y).collect())\n",
    "    mean_list = sorted([ value[1]/data.count() for value in max_min.reduceByKey(lambda x,y: x + y).collect()])\n",
    "    \n",
    "    return data.map(lambda x: [(float(x[index]) - min__list[index][1])/(max__list[index][1] - min__list[index][1]) if index != len(x)-1 else x[index] for index in range(len(x))] )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a tuple of sub-rdds and the cross-validation iteration index,\n",
    "#  this method returns a tuple containing training and validation rdds\n",
    "def get_train_validation_rdds(sub_rdds, k, indices=list(range(0, 4))):\n",
    "    \n",
    "    # the validation set is the k-th sub-rdd\n",
    "    validation_rdd = sub_rdds[indices.pop(k)]\n",
    "    \n",
    "    # initialize the train rdd with the first sub-rdd left\n",
    "    train_rdd = sub_rdds[indices.pop(0)]\n",
    "    \n",
    "    # append all the remaining sub-rdds to the train-rdd\n",
    "    for i in indices:\n",
    "        train_rdd = train_rdd.union(sub_rdds[i])\n",
    "    \n",
    "    # save train and validation set in a file\n",
    "    validation_rdd.saveAsTextFile('spam.validation' + str(k+1) + '.norm.data')\n",
    "    train_rdd.saveAsTextFile('spam.train' + str(k+1) + '.data')\n",
    "    \n",
    "    return train_rdd, validation_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is the features vector without label\n",
    "# w is the weights vector\n",
    "# b is the bias\n",
    "def predict(w, x, b):\n",
    "    return (1 / (1 + math.exp(-(np.dot(w, x)+b))))\n",
    "\n",
    "def get_cost_upd(x_y_yhat):\n",
    "    x, y, yhat = x_y_yhat\n",
    "    return y * math.log(yhat) + (1-y) * math.log(1-yhat)\n",
    "\n",
    "def get_weight_upd(x_y_yhat, j):\n",
    "    x, y, yhat = x_y_yhat\n",
    "    return (yhat - y) * x[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"Spam Filter\").setMaster(\"local[1]\").set(\"spark.hadoop.validateOutputSpecs\", \"false\");\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "file_object  = open('spam.data', 'r')\n",
    "lines = file_object.readlines()\n",
    "file_object.close()\n",
    "    \n",
    "total_size = len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RDD\n",
    "master_rdd = sc.parallelize(lines)\n",
    "master_rdd = master_rdd.map(lambda x: [float(item) for item in x.split('\\n')[0].split(\" \")])\n",
    "master_norm_rdd = normalize_data(master_rdd)\n",
    "master_norm_rdd = master_norm_rdd.\\\n",
    "map(lambda x: (x[:-1], x[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.0, 0.04481792717086835, 0.12549019607843137, 0.0, 0.032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06618407445708377, 0.0, 0.0, 0.0, 0.016, 0.0, 0.1419141914191419, 0.10293333333333334, 0.0, 0.08640864086408641, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02395467701213129, 0.0, 0.0, 0.0025020426690876077, 0.006007208650380457, 0.01748737373737374], 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(master_norm_rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the original rdd in non-test and test rdds\n",
    "non_test_rdd, test_rdd = master_norm_rdd.randomSplit([0.8, 0.2])\n",
    "non_test_rdd = non_test_rdd.cache()\n",
    "test_rdd = test_rdd.cache()\n",
    "\n",
    "# save test set in a file\n",
    "test_rdd.saveAsTextFile('spam.test.set')\n",
    "\n",
    "# divide the non-test rdd in 4 sub-rdds\n",
    "sub_rdds = non_test_rdd.randomSplit([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "# k-fold iterations\n",
    "for k in range(0, 4):\n",
    "    # for every iteration get a different train and validation sets\n",
    "    train_rdd, validation_rdd = get_train_validation_rdds(sub_rdds, k, indices=list(range(0,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = train_rdd.first()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Features:  57\n",
      "( 0 ) Cost:  0.69314718056\n",
      "( 50 ) Cost:  3.38360778457\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-84f8e222500d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m                                for j in range(n)])\\\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mu1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mu1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mu2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mj_weightsumupds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mj_weightsumupds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msortByKey\u001b[0;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mmaxSampleSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumPartitions\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20.0\u001b[0m  \u001b[0;31m# constant from Spark's RangePartitioner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0mfraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxSampleSize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrddSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeyfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_rdd = train_rdd.cache()\n",
    "\n",
    "# compute useful constants for further computations\n",
    "m = train_rdd.count()\n",
    "alpha = 1\n",
    "lambdareg = 0 \n",
    "learnrate = alpha/m\n",
    "\n",
    "# initialize the true labels vector\n",
    "n = len(train_rdd.first()[0]) \n",
    "print(\"#Features: \", n)\n",
    "\n",
    "# initialize the weights vector (one weight per feature) and bias\n",
    "new_weights = np.zeros(n)\n",
    "new_bias = 0\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "for epoch in range(400):\n",
    "    \n",
    "    weights = new_weights\n",
    "    bias = new_bias\n",
    "\n",
    "    #FIRST STEP: compute the predictions for the given weights and append them to the rest\n",
    "    # REMEMBER: every row of the rdd is now a tuple (feature_vector, true_label)\n",
    "    xs_ys_yhats_rdd = train_rdd\\\n",
    "    .map(lambda x_y: x_y + (predict(weights, x_y[0], bias),))\\\n",
    "    .cache()\n",
    "\n",
    "    #SECOND STEP: compute the total cost for the computed predictions\n",
    "    cost = xs_ys_yhats_rdd\\\n",
    "    .map(lambda x_y_yhat: get_cost_upd(x_y_yhat))\\\n",
    "    .reduce(lambda c1, c2: c1+c2)\n",
    "\n",
    "    # (regularization)\n",
    "    cost_reg_term = lambdareg/(2*m) + sum([w**2 for w in weights])\n",
    "    cost = -1/m * cost - cost_reg_term\n",
    "    \n",
    "    if (epoch % 50 == 0):\n",
    "        print(\"(\", epoch, \") Cost: \", cost)\n",
    "\n",
    "    #THIRD STEP: update all the weights simoultaneously\n",
    "    # 3.1. get the updating term for all the weights\n",
    "    weights_upds = xs_ys_yhats_rdd\\\n",
    "    .flatMap(lambda x_y_yhat: [(j, get_weight_upd(x_y_yhat, j))\n",
    "                               for j in range(n)])\\\n",
    "    .reduceByKey(lambda u1, u2: u1+u2)\\\n",
    "    .sortByKey(True)\\\n",
    "    .map(lambda j_weightsumupds: j_weightsumupds[1])\\\n",
    "    .collect()\n",
    "    \n",
    "    bias_upd = xs_ys_yhats_rdd\\\n",
    "    .map(lambda x_y_yhat: x_y_yhat[1] - x_y_yhat[2])\\\n",
    "    .reduce(lambda p, q: p+q)\n",
    "    \n",
    "    # 3.2. update the old weights (with regularization)\n",
    "    weight_reg_term = (1 - learnrate * lambdareg)\n",
    "    new_weights = [weight * weight_reg_term - learnrate * weight_upd \n",
    "                   for weight, weight_upd in zip(weights, weights_upds)]\n",
    "    \n",
    "    #new_bias = bias * weight_reg_term - alpha / m * bias_upd\n",
    "    \n",
    "end = time.time()\n",
    "print()\n",
    "print(\"Cost: \", cost)\n",
    "print(\"Weights: \", weights)\n",
    "print(\"> Total elapsed time: \", ((end-start)/60), \"mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.657297925675477e-05"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([w**2 for w in weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11011574921452866, -0.08202888396314452, -0.2646418668477493, -0.007716386559429697, -0.14480315529986587, -0.07913141511726622, -0.06712535394819681, -0.041916883523459604, -0.07737248899535557, -0.06196593646912925, -0.10884987416294134, -0.2707127262802904, -0.0826885506083501, -0.028096957310303174, -0.04754759266570256, -0.05815633544087493, -0.09125081895242591, -0.09436598446613169, -0.4223064595109707, -0.022491868572623263, -0.35287101216875605, -0.027346082066765354, -0.09155955555383025, -0.03089527263324255, -0.13328976292967826, -0.07967519334763604, -0.1193444574469707, -0.07170660575625132, -0.03342924566348147, -0.09220464241546922, -0.026291505097520138, -0.05433271936024353, -0.025002606428841154, -0.05490243631059169, -0.02740413556437993, -0.06601693713837695, -0.09449137613593567, -0.004501834910314092, -0.03349729115736168, -0.06989412781137365, -0.03217378569361468, -0.042834758457191646, -0.06398311330810647, -0.015424224002466482, -0.06643515137504843, -0.042012266697278554, -0.014922229116990215, -0.013074169234413595, -0.03625406510273324, -0.06738139539195002, -0.018186055149021253, -0.038348900502726324, -0.05503824812294007, -0.009356810779012903, -0.016890793257553913, -0.021273794352099314]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(weights_upds)\n",
    "#print(\" 1st el (\", type(f[0]), \"): \", f[0])\n",
    "#print(\" 2nd el (\", type(f[1]), \"): \", f[1])\n",
    "#print(\"Keys: \", len(set(weights_upds.keys().collect())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Labels:  2793\n",
      "#Features:  57\n"
     ]
    }
   ],
   "source": [
    "\n",
    ".reduceByKey(lambda xj_y_yhat1, xj_y_yhat2: \n",
    "             get_weight_upd(xj_y_yhat1, xj_y_yhat2))\\\n",
    "\n",
    ".map(lambda j_weightsupds: - l_rate_over_size * j_weightsupds[1])\\\n",
    ".collect()\n",
    "\n",
    "new_weights = [sum(_) for _ in zip(weights, weights_upds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
