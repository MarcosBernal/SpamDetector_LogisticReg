{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time, random\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from pyspark import SparkContext, SparkConf, rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object  = open('spam.data', 'r')\n",
    "lines = file_object.readlines()\n",
    "file_object.close()\n",
    "    \n",
    "total_size = len(lines)\n",
    "\n",
    "conf = SparkConf().setAppName(\"Spam Filter\").setMaster(\"local[1]\").set(\"spark.hadoop.validateOutputSpecs\", \"false\");\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RDD\n",
    "master_rdd = sc.parallelize(lines)\n",
    "master_rdd = master_rdd.map(lambda x: [float(item) for item in x.split('\\n')[0].split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize values\n",
    "def normalize_data(data):\n",
    "    max_min = data.flatMap(lambda x: [ (index_key, x[index_key]) for index_key in range(len(x)-1)]) #Last position is label\n",
    "    max__list = sorted(max_min.reduceByKey(lambda x,y: x if x > y else y).collect())\n",
    "    min__list = sorted(max_min.reduceByKey(lambda x,y: x if x < y else y).collect())\n",
    "    mean_list = sorted([ value[1]/data.count() for value in max_min.reduceByKey(lambda x,y: x + y).collect()])\n",
    "    \n",
    "    return data.map(lambda x: [(float(x[index]) - min__list[index][1])/(max__list[index][1] - min__list[index][1]) if index != len(x)-1 else x[index] for index in range(len(x))] )\n",
    "    \n",
    "\n",
    "    \n",
    "master_norm_rdd = normalize_data(master_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a tuple of sub-rdds and the cross-validation iteration index,\n",
    "#  this method returns a tuple containing training and validation rdds\n",
    "def get_train_validation_rdds(sub_rdds, k, indices=list(range(0, 4))):\n",
    "    \n",
    "    # the validation set is the k-th sub-rdd\n",
    "    validation_rdd = sub_rdds[indices.pop(k)]\n",
    "    \n",
    "    # initialize the train rdd with the first sub-rdd left\n",
    "    train_rdd = sub_rdds[indices.pop(0)]\n",
    "    \n",
    "    # append all the remaining sub-rdds to the train-rdd\n",
    "    for i in indices:\n",
    "        train_rdd = train_rdd.union(sub_rdds[i])\n",
    "    \n",
    "    # save train and validation set in a file\n",
    "    validation_rdd.saveAsTextFile('spam.validation' + str(k+1) + '.norm.data')\n",
    "    train_rdd.saveAsTextFile('spam.train' + str(k+1) + '.data')\n",
    "    \n",
    "    validation_rdd = validation_rdd.cache()\n",
    "    train_rdd = train_rdd.cache()\n",
    "    \n",
    "    return train_rdd, validation_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# divide the original rdd in non-test and test rdds\n",
    "non_test_rdd, test_rdd = master_norm_rdd.randomSplit([0.8, 0.2])\n",
    "non_test_rdd = non_test_rdd.cache()\n",
    "test_rdd = test_rdd.cache()\n",
    "\n",
    "# save test set in a file\n",
    "test_rdd.saveAsTextFile('spam.test.set')\n",
    "\n",
    "# divide the non-test rdd in 4 sub-rdds\n",
    "sub_rdds = non_test_rdd.randomSplit([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "# k-fold iterations\n",
    "for k in range(0, 4):\n",
    "    # for every iteration get a different train and validation sets\n",
    "    train_rdd, validation_rdd = get_train_validation_rdds(sub_rdds, k, indices=list(range(0,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is the features vector without label\n",
    "# w is the weights vector\n",
    "# b is the bias\n",
    "def predict(w, x, b):\n",
    "    return (1 / (1 + math.exp(-(np.dot(w, x)+b))))\n",
    "\n",
    "def get_cost_upd(x_y_yhat):\n",
    "    x, y, yhat = x_y_yhat\n",
    "    return y * math.log(yhat) + (1-y) * math.log(1-yhat)\n",
    "\n",
    "def get_weight_upd(x_y_yhat, j):\n",
    "    x, y, yhat = x_y_yhat\n",
    "    return (yhat - y) * x[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Features:  57\n",
      "( 0 ) Cost:  0.69314718056\n",
      "( 1 ) Cost:  0.693033173087\n",
      "( 2 ) Cost:  0.692944596023\n",
      "( 3 ) Cost:  0.692881415797\n",
      "( 4 ) Cost:  0.692843598882\n",
      "( 5 ) Cost:  0.692831111799\n",
      "( 6 ) Cost:  0.692843921114\n",
      "( 7 ) Cost:  0.692881993441\n",
      "( 8 ) Cost:  0.692945295442\n",
      "( 9 ) Cost:  0.693033793822\n",
      "( 10 ) Cost:  0.693147455336\n",
      "( 11 ) Cost:  0.693286246784\n",
      "( 12 ) Cost:  0.693450135014\n",
      "( 13 ) Cost:  0.69363908692\n",
      "( 14 ) Cost:  0.693853069442\n",
      "( 15 ) Cost:  0.694092049569\n",
      "( 16 ) Cost:  0.694355994336\n",
      "( 17 ) Cost:  0.694644870823\n",
      "( 18 ) Cost:  0.69495864616\n",
      "( 19 ) Cost:  0.69529728752\n",
      "( 20 ) Cost:  0.695660762128\n",
      "( 21 ) Cost:  0.696049037251\n",
      "( 22 ) Cost:  0.696462080207\n",
      "( 23 ) Cost:  0.696899858357\n",
      "( 24 ) Cost:  0.697362339112\n",
      "( 25 ) Cost:  0.69784948993\n",
      "( 26 ) Cost:  0.698361278313\n",
      "( 27 ) Cost:  0.698897671814\n",
      "( 28 ) Cost:  0.69945863803\n",
      "( 29 ) Cost:  0.700044144606\n",
      "( 30 ) Cost:  0.700654159235\n",
      "( 31 ) Cost:  0.701288649655\n",
      "( 32 ) Cost:  0.701947583654\n",
      "( 33 ) Cost:  0.702630929063\n",
      "( 34 ) Cost:  0.703338653765\n",
      "( 35 ) Cost:  0.704070725686\n",
      "( 36 ) Cost:  0.704827112801\n",
      "( 37 ) Cost:  0.705607783131\n",
      "( 38 ) Cost:  0.706412704747\n",
      "( 39 ) Cost:  0.707241845762\n",
      "( 40 ) Cost:  0.708095174341\n",
      "( 41 ) Cost:  0.708972658694\n",
      "( 42 ) Cost:  0.709874267078\n",
      "( 43 ) Cost:  0.710799967797\n",
      "( 44 ) Cost:  0.711749729204\n",
      "( 45 ) Cost:  0.712723519695\n",
      "( 46 ) Cost:  0.713721307719\n",
      "( 47 ) Cost:  0.714743061767\n",
      "( 48 ) Cost:  0.715788750379\n",
      "( 49 ) Cost:  0.716858342142\n",
      "( 50 ) Cost:  0.717951805692\n",
      "( 51 ) Cost:  0.719069109709\n",
      "( 52 ) Cost:  0.720210222922\n",
      "( 53 ) Cost:  0.721375114106\n",
      "( 54 ) Cost:  0.722563752085\n",
      "( 55 ) Cost:  0.723776105728\n",
      "( 56 ) Cost:  0.725012143952\n",
      "( 57 ) Cost:  0.726271835721\n",
      "( 58 ) Cost:  0.727555150046\n",
      "( 59 ) Cost:  0.728862055986\n",
      "( 60 ) Cost:  0.730192522646\n",
      "( 61 ) Cost:  0.731546519177\n",
      "( 62 ) Cost:  0.732924014781\n",
      "( 63 ) Cost:  0.734324978702\n",
      "( 64 ) Cost:  0.735749380235\n",
      "( 65 ) Cost:  0.73719718872\n",
      "( 66 ) Cost:  0.738668373545\n",
      "( 67 ) Cost:  0.740162904143\n",
      "( 68 ) Cost:  0.741680749998\n",
      "( 69 ) Cost:  0.743221880636\n",
      "( 70 ) Cost:  0.744786265634\n",
      "( 71 ) Cost:  0.746373874614\n",
      "( 72 ) Cost:  0.747984677246\n",
      "( 73 ) Cost:  0.749618643245\n",
      "( 74 ) Cost:  0.751275742375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-59265ebca98f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                                for j in range(n_features)])\\\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mu1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mu1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mu2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mj_weightsumupds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mj_weightsumupds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msortByKey\u001b[0;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;31m# the key-space into bins such that the bins have roughly the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;31m# number of (key, value) pairs falling into them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0mrddSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrddSize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m  \u001b[0;31m# empty RDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \"\"\"\n\u001b[0;32m-> 1041\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \"\"\"\n\u001b[0;32m-> 1032\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0;31m# Write will only fail if remote is closed for large payloads or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m             \u001b[0;31m# if it sent a RST packet (SO_LINGER)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error while sending.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_rdd = train_rdd.cache()\n",
    "\n",
    "# compute useful constants for further computations\n",
    "m = train_rdd.count()\n",
    "alpha = 0.1\n",
    "lambdareg = 0 \n",
    "\n",
    "# initialize the true labels vector\n",
    "n_features = len(train_rdd.first()) - 1 # do not consider the true label\n",
    "print(\"#Features: \", n_features)\n",
    "\n",
    "# initialize the weights vector (one weight per feature) and bias\n",
    "new_weights = np.zeros(n_features)\n",
    "new_bias = 0\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "for epoch in range(400):\n",
    "    \n",
    "    weights = new_weights\n",
    "    bias = new_bias\n",
    "\n",
    "    #FIRST STEP: compute the predictions for the given weights\n",
    "    xs_ys_yhats_rdd = train_rdd\\\n",
    "    .map(lambda x: (x[:-1], x[-1], predict(weights, x[:-1], bias)))\\\n",
    "    .cache()\n",
    "\n",
    "    #SECOND STEP: compute the total cost for the computed predictions\n",
    "    cost = xs_ys_yhats_rdd\\\n",
    "    .map(lambda x_y_yhat: get_cost_upd(x_y_yhat))\\\n",
    "    .reduce(lambda c1, c2: c1+c2)\n",
    "\n",
    "    # (regularization)\n",
    "    cost_reg_term = lambdareg/(2*m) + sum([w**2 for w in weights])\n",
    "    cost = -1/m * cost + cost_reg_term\n",
    "    \n",
    "    if (epoch % 50 == 0)\n",
    "        print(\"(\", epoch, \") Cost: \", cost)\n",
    "\n",
    "    #THIRD STEP: update all the weights simoultaneously\n",
    "    # 3.1. get the updating term for all the weights\n",
    "    weights_upds = xs_ys_yhats_rdd\\\n",
    "    .flatMap(lambda x_y_yhat: [(j, get_weight_upd(x_y_yhat, j))\n",
    "                               for j in range(n_features)])\\\n",
    "    .reduceByKey(lambda u1, u2: u1+u2)\\\n",
    "    .sortByKey(True)\\\n",
    "    .map(lambda j_weightsumupds: - alpha / m * j_weightsumupds[1])\\\n",
    "    .collect()\n",
    "    \n",
    "    # 3.2. update the old weights (with regularization)\n",
    "    weight_reg_term = (1 - alpha * lambdareg / m)\n",
    "    new_weights = [weight * weight_reg_term + weight_upd \n",
    "                   for weight, weight_upd in zip(weights, weights_upds)]\n",
    "    \n",
    "end = time.time()\n",
    "print()\n",
    "print(\"Cost: \", cost)\n",
    "print(\"Weights: \", weights)\n",
    "print(\"> Total elapsed time: \", ((end-start)/60), \"mins\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.657297925675477e-05"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([w**2 for w in weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11011574921452866, -0.08202888396314452, -0.2646418668477493, -0.007716386559429697, -0.14480315529986587, -0.07913141511726622, -0.06712535394819681, -0.041916883523459604, -0.07737248899535557, -0.06196593646912925, -0.10884987416294134, -0.2707127262802904, -0.0826885506083501, -0.028096957310303174, -0.04754759266570256, -0.05815633544087493, -0.09125081895242591, -0.09436598446613169, -0.4223064595109707, -0.022491868572623263, -0.35287101216875605, -0.027346082066765354, -0.09155955555383025, -0.03089527263324255, -0.13328976292967826, -0.07967519334763604, -0.1193444574469707, -0.07170660575625132, -0.03342924566348147, -0.09220464241546922, -0.026291505097520138, -0.05433271936024353, -0.025002606428841154, -0.05490243631059169, -0.02740413556437993, -0.06601693713837695, -0.09449137613593567, -0.004501834910314092, -0.03349729115736168, -0.06989412781137365, -0.03217378569361468, -0.042834758457191646, -0.06398311330810647, -0.015424224002466482, -0.06643515137504843, -0.042012266697278554, -0.014922229116990215, -0.013074169234413595, -0.03625406510273324, -0.06738139539195002, -0.018186055149021253, -0.038348900502726324, -0.05503824812294007, -0.009356810779012903, -0.016890793257553913, -0.021273794352099314]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(weights_upds)\n",
    "#print(\" 1st el (\", type(f[0]), \"): \", f[0])\n",
    "#print(\" 2nd el (\", type(f[1]), \"): \", f[1])\n",
    "#print(\"Keys: \", len(set(weights_upds.keys().collect())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Labels:  2793\n",
      "#Features:  57\n"
     ]
    }
   ],
   "source": [
    "\n",
    ".reduceByKey(lambda xj_y_yhat1, xj_y_yhat2: \n",
    "             get_weight_upd(xj_y_yhat1, xj_y_yhat2))\\\n",
    "\n",
    ".map(lambda j_weightsupds: - l_rate_over_size * j_weightsupds[1])\\\n",
    ".collect()\n",
    "\n",
    "new_weights = [sum(_) for _ in zip(weights, weights_upds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
