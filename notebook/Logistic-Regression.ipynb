{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Table of Contents\n",
    "1.  [Introduction](#introduction)\n",
    "2.  [Dataset](#dataset)\n",
    "     1. [Attribute Analysis](#attribute-analysis)\n",
    "3.  [Logistic Regression](#logistic-regression)\n",
    "     1. [Cost Function](#cost-function)\n",
    "     2. [Gradient Descent](#gradient-descent)\n",
    "4.  [Cross Validation Procedure](#cross-validation-procedure)\n",
    "5.  [Implementation](#implementation)\n",
    "     1. [Normalization](#normalization)\n",
    "     2. [Data set splitting](#data set splitting)\n",
    "     3. [Cost Function Implementation](#cost-function-implementation)\n",
    "     4. [Gradient Descent Implementation](#cost-function-implementation)\n",
    "6.  [Experiments](#experiments)\n",
    "    1. [Performance measures](#performance measures)\n",
    "    2. [Comparison](#benchmarks)\n",
    "          1. [Standard libraries](#standard libraries)\n",
    "          2. [Numpy version](#comparison-with-numpy)\n",
    "     \n",
    "7.  [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    " \n",
    "\n",
    "More information at:\n",
    "- [Machine Learning Coursera - Andre Ng](https://www.coursera.org/learn/machine-learning/)\n",
    "- [Lecture Notes pg(16-19)](http://cs229.stanford.edu/notes/cs229-notes1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Spambase dataset contains 4601 instances of different emails classified into spam or legit email according 57 continious values plus the nominal class which is the label. \n",
    "\n",
    "The dataset was produced by Hewlett-Packard Labs in Juky 1999.\n",
    "\n",
    "The dataset is uploaded in [this repository](ftp://ftp.ics.uci.edu/pub/machine-learning-databases/spambase/).\n",
    "\n",
    "### Attribute analysis\n",
    "\n",
    "All attributes are continuous which make suitable to use regression, in our case, logistic regression as we want to detect if there is spam(dependent variable) depending of a set of attributes(predictors or independent variables).\n",
    "\n",
    "The attribute statistics are summarized in this tabble:\n",
    "\n",
    "| Column |Min: |Max: |Average: |Std.Dev: |Coeff.Var_%: |\n",
    "|--|--|-----|--------|--------|----|\n",
    "|1 |0 |4.54 |0.10455 |0.30536 |292 |\n",
    "|2 |0 |14.28 |0.21301 |1.2906 |606 |\n",
    "|3 |0 |5.1 |0.28066 |0.50414 |180 |\n",
    "|4 |0 |42.81 |0.065425 |1.3952 |2130 |\n",
    "|5 |0 |10 |0.31222 |0.67251 |215 |\n",
    "|6 |0 |5.88 |0.095901 |0.27382 |286 |\n",
    "|7 |0 |7.27 |0.11421 |0.39144 |343 |\n",
    "|8 |0 |11.11 |0.10529 |0.40107 |381 |\n",
    "|9 |0 |5.26 |0.090067 |0.27862 |309 |\n",
    "|10 |0 |18.18 |0.23941 |0.64476 |269 |\n",
    "|11 |0 |2.61 |0.059824 |0.20154 |337 |\n",
    "|12 |0 |9.67 |0.5417 |0.8617 |159 |\n",
    "|13 |0 |5.55 |0.09393 |0.30104 |320 |\n",
    "|14 |0 |10 |0.058626 |0.33518 |572 |\n",
    "|15 |0 |4.41 |0.049205 |0.25884 |526 |\n",
    "|16 |0 |20 |0.24885 |0.82579 |332 |\n",
    "|17 |0 |7.14 |0.14259 |0.44406 |311 |\n",
    "|18 |0 |9.09 |0.18474 |0.53112 |287 |\n",
    "|19 |0 |18.75 |1.6621 |1.7755 |107 |\n",
    "|20 |0 |18.18 |0.085577 |0.50977 |596 |\n",
    "|21 |0 |11.11 |0.80976 |1.2008 |148 |\n",
    "|22 |0 |17.1 |0.1212 |1.0258 |846 |\n",
    "|23 |0 |5.45 |0.10165 |0.35029 |345 |\n",
    "|24 |0 |12.5 |0.094269 |0.44264 |470 |\n",
    "|25 |0 |20.83 |0.5495 |1.6713 |304 |\n",
    "|26 |0 |16.66 |0.26538 |0.88696 |334 |\n",
    "|27 |0 |33.33 |0.7673 |3.3673 |439 |\n",
    "|28 |0 |9.09 |0.12484 |0.53858 |431 |\n",
    "|29 |0 |14.28 |0.098915 |0.59333 |600 |\n",
    "|30 |0 |5.88 |0.10285 |0.45668 |444 |\n",
    "|31 |0 |12.5 |0.064753 |0.40339 |623 |\n",
    "|32 |0 |4.76 |0.047048 |0.32856 |698 |\n",
    "|33 |0 |18.18 |0.097229 |0.55591 |572 |\n",
    "|34 |0 |4.76 |0.047835 |0.32945 |689 |\n",
    "|35 |0 |20 |0.10541 |0.53226 |505 |\n",
    "|36 |0 |7.69 |0.097477 |0.40262 |413 |\n",
    "|37 |0 |6.89 |0.13695 |0.42345 |309 |\n",
    "|38 |0 |8.33 |0.013201 |0.22065 |1670 |\n",
    "|39 |0 |11.11 |0.078629 |0.43467 |553 |\n",
    "|40 |0 |4.76 |0.064834 |0.34992 |540 |\n",
    "|41 |0 |7.14 |0.043667 |0.3612 |827 |\n",
    "|42 |0 |14.28 |0.13234 |0.76682 |579 |\n",
    "|43 |0 |3.57 |0.046099 |0.22381 |486 |\n",
    "|44 |0 |20 |0.079196 |0.62198 |785 |\n",
    "|45 |0 |21.42 |0.30122 |1.0117 |336 |\n",
    "|46 |0 |22.05 |0.17982 |0.91112 |507 |\n",
    "|47 |0 |2.17 |0.0054445 |0.076274 |1400 |\n",
    "|48 |0 |10 |0.031869 |0.28573 |897 |\n",
    "|49 |0 |4.385 |0.038575 |0.24347 |631 |\n",
    "|50 |0 |9.752 |0.13903 |0.27036 |194 |\n",
    "|51 |0 |4.081 |0.016976 |0.10939 |644 |\n",
    "|52 |0 |32.478 |0.26907 |0.81567 |303 |\n",
    "|53 |0 |6.003 |0.075811 |0.24588 |324 |\n",
    "|54 |0 |19.829 |0.044238 |0.42934 |971 |\n",
    "|55 |1 |1102.5 |5.1915 |31.729 |611 |\n",
    "|56 |1 |9989 |52.173 |194.89 |374 |\n",
    "|57 |1 |15841 |283.29 |606.35 |214 |\n",
    "|58 |0 |1 |0.39404 |0.4887 |124 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression(LR) is an algorithm used in supervised problems where the data is labeled previously and it is needed a binary or dichotomous classifier.\n",
    "\n",
    "The LR algorithm it is based on linear regression and the logistic function...\n",
    "\n",
    "### Cost function\n",
    "\n",
    "The cost function applied in logistic regression \n",
    "\n",
    "\n",
    "$Cost(h_{\\theta}(x),y)=\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  -log({h_{\\theta}(x)})    & \\quad \\text{if } x \\text{ is 1}\\\\\n",
    "                  -log({1-h_{\\theta}(x)})  & \\quad \\text{if } n \\text{ is 0}\n",
    "                \\end{array}\n",
    "              \\right.$\n",
    "              \n",
    "Intuitively: the cost funtions returns 0 if the hypothesis function output is equal to the true label. In case the model states a diferent output the cost funtion will approach infinity.\n",
    "              \n",
    "If we reduce the formula in a more compact way we get: \n",
    "$Cost(h_{\\theta}(x),y) = -y·log(h_{\\theta}(x)) - (1-y)log(1-h_{\\theta}(x))$\n",
    "\n",
    "\n",
    "Info obtained from [lecture of Andrew NG](https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Procedure\n",
    "\n",
    "\n",
    "To ensure we correctly train, validate and test our model, we will divide our dataset in three main components with such proportions:\n",
    "\n",
    "- __Training set (~ 60%)__: this data will be used to train a given model\n",
    "- __Validation set (~ 20%)__: this data will be used to evaluate a given trained model\n",
    "- __Test set (~ 20%)__: this data will be used to evaluate the ultimately chosen model.\n",
    "\n",
    "Also, to reduce the chances of overfitting, we will use cross-validation. To respect these splitting ratios, we will use a __4-fold cross-validation__ so that 20% of the original dataset (25% of the non-testing set) is always devoted to validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster configuration and RDD creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object  = open('spam.data', 'r')\n",
    "lines = file_object.readlines()\n",
    "file_object.close()\n",
    "    \n",
    "total_size = len(lines)\n",
    "\n",
    "import math, time\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from pyspark import SparkContext, SparkConf, rdd\n",
    "conf = SparkConf().setAppName(\"Spam Filter\").setMaster(\"local[1]\").set(\"spark.hadoop.validateOutputSpecs\", \"false\");\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RDD\n",
    "master_rdd = sc.parallelize(lines)\n",
    "master_rdd = master_rdd.map(lambda x: [float(item) for item in x.split('\\n')[0].split(\" \")])\n",
    "master_rdd = master_rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "Calculated stats of the features of all instances: max, min, mean, std_deviation, and coeff_variation\n",
      "Values have been normalized and are kept in the 'master_norm_rdd' variable\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "#temporal_list = master_rdd.collect()\n",
    "#shuffle(temporal_list) # Labels are all together 1's and 0's\n",
    "#master_rdd = sc.parallelize(temporal_list)\n",
    "\n",
    "# Get stats of the instance values\n",
    "max_min_rdd = master_rdd.flatMap(lambda x: [ (index_key, x[index_key]) for index_key in range(len(x))])\n",
    "max_list = [ item[1] for item in max_min_rdd.reduceByKey(lambda x,y: x if x > y else y).collect()]\n",
    "min_list = [ item[1] for item in max_min_rdd.reduceByKey(lambda x,y: x if x < y else y).collect()]\n",
    "mean_list = [ value[1]/len(lines) for value in max_min_rdd.reduceByKey(lambda x,y: x + y).collect()]\n",
    "std_deviation_list = [ math.sqrt((value[1]/len(lines))) for value in max_min_rdd.map(lambda x:  [ x[0],(x[1] - mean_list[x[0]])*(x[1] - mean_list[x[0]]) ]).reduceByKey(lambda x,y: x+y).collect() ]\n",
    "coeff_variation = [ std_deviation_list[index]/mean_list[index] for index in range(len(std_deviation_list))]\n",
    "\n",
    "# Normalize values\n",
    "def normalize_data(data):\n",
    "    max_min = data.flatMap(lambda x: [ (index_key, x[index_key]) for index_key in range(len(x)-1)]) #Last position is label\n",
    "    max__list = sorted(max_min.reduceByKey(lambda x,y: x if x > y else y).collect())\n",
    "    min__list = sorted(max_min.reduceByKey(lambda x,y: x if x < y else y).collect())\n",
    "    mean_list = sorted([ value[1]/data.count() for value in max_min.reduceByKey(lambda x,y: x + y).collect()])\n",
    "    \n",
    "    return data.map(lambda x: [(float(x[index]) - min__list[index][1])/(max__list[index][1] - min__list[index][1]) if index != len(x)-1 else x[index] for index in range(len(x))] )\n",
    "    \n",
    "\n",
    "    \n",
    "master_norm_rdd = normalize_data(master_rdd).cache()\n",
    "#.map(lambda x: [(x[index] - min_list[index])/(max_list[index] - min_list[index]) for index in range(len(x)-1)] )\n",
    "max_min_norm_rdd = master_norm_rdd.flatMap(lambda x: [ (index_key, x[index_key]) for index_key in range(len(x))])\n",
    "\n",
    "print(\"--\")\n",
    "print(\"Calculated stats of the features of all instances: max, min, mean, std_deviation, and coeff_variation\")\n",
    "print(\"Values have been normalized and are kept in the \\'master_norm_rdd\\' variable\")\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split of data in different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train & Validation:  3696  samples\n",
      "Test:  905  samples\n",
      "> Do sizes match?  True\n"
     ]
    }
   ],
   "source": [
    "non_test_rdd, test_rdd = master_norm_rdd.randomSplit([0.8, 0.2])\n",
    "non_test_rdd = non_test_rdd.cache()\n",
    "test_rdd = test_rdd.cache()\n",
    "\n",
    "# save test set in a file\n",
    "test_rdd.saveAsTextFile('spam.test.set')\n",
    "\n",
    "print(\"Train & Validation: \", non_test_rdd.count(), \" samples\")\n",
    "print(\"Test: \", test_rdd.count(), \" samples\")\n",
    "print(\"> Do sizes match? \", non_test_rdd.count() + test_rdd.count() == total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_validation_rdds(sub_rdds, k):\n",
    "    \n",
    "    indices = list(range(0, 4))\n",
    "    \n",
    "    # the validation set is the k-th sub-rdd\n",
    "    validation_rdd = sub_rdds[indices.pop(k)]\n",
    "    \n",
    "    # initialize the train rdd with the first sub-rdd left\n",
    "    train_rdd = sub_rdds[indices.pop(0)]\n",
    "    # append all the remaining sub-rdds to the train-rdd\n",
    "    for i in indices:\n",
    "        train_rdd = train_rdd.union(sub_rdds[i])\n",
    "    \n",
    "    # save train and validation set in a file\n",
    "    validation_rdd.saveAsTextFile('spam.validation' + str(k+1) + '.norm.data')\n",
    "    train_rdd.saveAsTextFile('spam.train' + str(k+1) + '.data')\n",
    "    \n",
    "    validation_rdd = validation_rdd.cache()\n",
    "    train_rdd = train_rdd.cache()\n",
    "    \n",
    "    return train_rdd, validation_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sub_rdds = non_test_rdd.randomSplit([0.25, 0.25, 0.25, 0.25])\n",
    "for rdd in sub_rdds:\n",
    "    rdd = rdd.cache()\n",
    "\n",
    "# k-fold iterations\n",
    "for k in range(0, 4):\n",
    "    \n",
    "    train_rdd, validation_rdd = get_train_validation_rdds(sub_rdds, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'spam.data' has been normalized and splitted in 3 files:  \n",
      " 'spam.training.norm.data', 'spam.validating.norm.data', 'spam.testing.norm.data'\n",
      "Sizes:  \n",
      " - Training data:  2761  samples \n",
      " - Validating data:  920  s. \n",
      " - Testing data:  920  s. \n",
      " -- Total size: 4601  s.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "validating_size = round(total_size*0.2)\n",
    "\n",
    "# Creating RDDs\n",
    "master_norm_list = master_norm_rdd.collect()\n",
    "\n",
    "# Save a portion of the RDD as validating set and use the rest for training and testing purposes\n",
    "validatingset_rdd = master_norm_rdd.sample\n",
    "train_and_test_rdd = sc.parallelize(master_norm_list[validating_size:])\n",
    "\n",
    "trainingset_rdd.saveAsTextFile('spam.training.norm.data')\n",
    "validatingset_rdd.saveAsTextFile('spam.validating.norm.data')\n",
    "testingset_rdd.saveAsTextFile('spam.testing.norm.data')\n",
    "\n",
    "print(\"File \\'spam.data\\' has been normalized and splitted in 3 files: \", \n",
    "      \"\\n \\'spam.training.norm.data\\', \\'spam.validating.norm.data\\', \\'spam.testing.norm.data\\'\")\n",
    "print(\"Sizes: \",\n",
    "      \"\\n - Training data: \", trainingset_rdd.count() , \" samples\",\n",
    "      \"\\n - Validating data: \",validatingset_rdd.count(), \" s.\",\n",
    "      \"\\n - Testing data: \",testingset_rdd.count(), \" s.\",\n",
    "      \"\\n -- Total size:\", total_size, \" s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "From the [previous chapter](#), the cost function was defined as:\n",
    "\n",
    "$Cost(h_{\\theta}(x),y) = -y·log(h_{\\theta}(x)) - (1-y)log(1-h_{\\theta}(x))$\n",
    "\n",
    "where $h_{\\theta}(x)$ is the predicted label and $y$ the true label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_logistic_regression_model(training_set, l_rate, iterations=1000, lambda_reg = 0):\n",
    "    # Init the coeff\n",
    "    n_features = len(training_set[0])\n",
    "    \n",
    "    weights = [0.0 for index in range(n_features)]\n",
    "    l_rate_over_m = l_rate/len(training_set)\n",
    "    \n",
    "    # See http://holehouse.org/mlclass/07_Regularization.html for details\n",
    "    for iteration in range(iterations):\n",
    "        sum_error = 0\n",
    "        weights_upd = [ [] for index in range(n_features)]\n",
    "        \n",
    "        for instance in training_set:\n",
    "            y = instance[-1]   # the last element of each row is the true label\n",
    "            yhat = predict(weights, instance)  # compute the predicted label according to current weights\n",
    "            \n",
    "            error = yhat - y\n",
    "            sum_error += error**2\n",
    "            \n",
    "            for i in range(len(instance)): # Adding items of sumatorium \n",
    "                if (i < len(instance) -1):\n",
    "                    weights_upd[i].append((error * instance[i]))\n",
    "                else:\n",
    "                    weights_upd[-1].append(error) # Calculating the bias \\theta_0 \n",
    "        \n",
    "        # Regularization\n",
    "        for i in range(n_features):  \n",
    "            weights[i] = weights[i] * (1 - l_rate_over_m * lambda_reg) - (l_rate_over_m * sum(weights_upd[i]))\n",
    "        \n",
    "    return ( sum_error, weights )\n",
    "\n",
    "## SPARK IMPLEMENTATIONS ###############################################################################################\n",
    "# coeff and values must be lists with the same length\n",
    "def predict(coeff, values):\n",
    "    \n",
    "    logit = sum([ coeff[index] * values[index] for index in range(len(coeff) -1) ]) + coeff[-1]\n",
    "    \n",
    "    if logit < 0: # when logit becomes a large positive value, math.exp(gamma) overflows\n",
    "        sigmoid = 1 - 1 / (1 + math.exp(logit))\n",
    "    else:\n",
    "        sigmoid = 1 / (1 + math.exp(-logit))\n",
    "    return sigmoid\n",
    "\n",
    "# https://www.coursera.org/learn/machine-learning/lecture/10sqI/map-reduce-and-data-parallelism\n",
    "import time\n",
    "def training_logistic_regression_model_spark(train_rdd, l_rate, iterations=1000, lambda_reg = 0):\n",
    "    \n",
    "    # compute useful constants for further computations\n",
    "    l_rate_over_size = l_rate / train_rdd.count()\n",
    "    n_features = len(train_rdd.first())\n",
    "\n",
    "    # initialize the weights vector (one weight per feature)\n",
    "    n_weights = n_features\n",
    "    weights = np.zeros(n_features)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        if (iteration % 10 == 0):\n",
    "            start_it = time.time()\n",
    "        \n",
    "        # the key of the <key, value> pairs is the index of the feature, \n",
    "        #  so that we can reduce by it and sum all the updates\n",
    "        weights = train_rdd \\\n",
    "        .flatMap(lambda instance: [(f, (weights[f], get_weight_upd(f, weights, instance))) for f in range(n_features)])\\\n",
    "        .reduceByKey(lambda x, y: (x[0], x[1] + y[1]))\\\n",
    "        .map(lambda key_weight_weightupd: update_weight(weight_weightupd=key_weight_weightupd[1],\n",
    "                                                        l_rate_over_size=l_rate_over_size, \n",
    "                                                        lambda_reg=0))\\\n",
    "        .collect()\n",
    "        \n",
    "        if (iteration % 10 == 0):\n",
    "            start_cf = time.time()\n",
    "            \n",
    "        cost =  cost_function_spark(weights, train_rdd, lambda_reg)\n",
    "        \n",
    "        if (iteration % 10 == 0):\n",
    "            print(\"> Cost function running time: \", time.time() - start_cf, \"s\")\n",
    "        \n",
    "        if(iteration % 10 == 0):\n",
    "            print('> iteration=',iteration,' lrate=',l_rate, ' cost_func=', cost)\n",
    "            print(\"> Iteration running time: \", time.time() - start_it, \"s\")\n",
    "            \n",
    "    return cost, weights\n",
    "\n",
    "def get_weight_upd(i_feature, weights, instance):\n",
    "    n_features = len(instance)\n",
    "    if (i_feature < n_features -1):\n",
    "        return (predict(weights, instance) - instance[-1]) * instance[i_feature]\n",
    "    else:\n",
    "        return (predict(weights, instance) - instance[-1])\n",
    "    \n",
    "    \n",
    "def cost_function_spark(weights, data_rdd, lambda_reg=0):\n",
    "    #if(not isinstance(dataset, rdd.PipelineRDD)):\n",
    "    #    dataset = sc.parallelize(dataset)\n",
    "\n",
    "    size = data_rdd.count()\n",
    "        \n",
    "    summation = data_rdd \\\n",
    "    .map(lambda instance: get_cost_upd(weights, instance)) \\\n",
    "    .reduce(lambda cost_upd1, cost_upd2: cost_upd1 + cost_upd2)\n",
    "    \n",
    "    reg_term = lambda_reg/size * sum([ weights[index]**2 for index in range(len(weights)-1) ]) \n",
    "    cost = -summation/size + reg_term\n",
    "    return cost\n",
    "\n",
    "def get_cost_upd(weights, instance):\n",
    "    y = instance[-1]                    # true label\n",
    "    yhat = predict(weights, instance)   # predicted label\n",
    "    \n",
    "    return y * math.log(yhat) + (1 - y) * math.log(1 - yhat)\n",
    "    \n",
    "\n",
    "def optimal_learning_rate_value_spark(training_set_rdd, initial_l_rate = 0.05, max_iterations = 20000, lambda_reg = 0):\n",
    "    order_log_list = [initial_l_rate]\n",
    "    [ order_log_list.append(order_log_list[-1]*(2)) for index in range(10)]\n",
    "    error_coeff_list = []\n",
    "    for l_rate in order_log_list:\n",
    "        error_coeff_list.append(\n",
    "            training_logistic_regression_model_spark(training_set_rdd, l_rate, max_iterations, lambda_reg))\n",
    "    return sorted(error_coeff_list)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weight(weight_weightupd, l_rate_over_size, lambda_reg):\n",
    "    weight, weight_upd = weight_weightupd\n",
    "    return (weight * (1 - l_rate_over_size * lambda_reg)) - (l_rate_over_size * weight_upd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0031901167258571073, 0.00082167436106014781, 0.014432547883848405, 0.0026106882044988427, 0.0031657644722971343, 0.0084599784637473044, 0.0071295525245273867, 0.0049993073135099035, -0.024840270207770219, -0.011647108716614663, -0.0045550610193826275, -0.0085820760464990035, -0.014687265896140458, -0.006751192939487353, -0.010706795280926438, -0.0069295294092750776, -0.0034667555074607442, 0.0041709718615863924, 0.0023122326501843261, -0.19956927494615936, -0.0048851198770965736, 0.0094256999282125026, 0.0057148930973499358, 0.01386929172408293, 0.0014479540559942578, 0.013431000540919875, 0.0047635237321291346, 0.018776055902052856, -0.015091320088695927, -0.0060081821673392943, -0.0086906621945260533, -0.0044061378320172278, -0.0014340265898175493, -0.0057626065501577509, -0.0037193108399138537, -0.0022330215924917544, -0.0054125146416484152, 0.010852673687085628, 0.0030591627397241247, 0.0088917979252002236, 0.0076866353145709091, 0.009425181311227512, -0.0092818947745077988, 0.010427908899860497, 0.0079365079365079326, 0.026294194387134338, 0.0062871500358937576, -0.023175540813234231, -0.015097352652475207, -0.0051682662882225052, -0.0087003847026326121, -0.0064705680905491839, -0.009689554213645259, -0.0092697045317289301, -0.0030520459440057427, -0.0024423760557258233, 0.0010033199505448537, 0.005936360192013458]\n",
      ">>  3.947946310043335 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "l_rate = 2\n",
    "l_rate_over_size = l_rate / train_rdd.count()\n",
    "n_features = len(train_rdd.first())\n",
    "weights1 = np.zeros(n_features)\n",
    "\n",
    "weights1 = train_rdd\\\n",
    ".flatMap(lambda instance: [(i_feature, (weights1[i_feature], get_weight_upd(i_feature, weights1, instance))) \n",
    "                           for i_feature in range(n_features)])\\\n",
    ".reduceByKey(lambda x, y: (x[0], x[1] + y[1]))\\\n",
    ".map(lambda key_weight_weightupd: update_weight(weight_weightupd=key_weight_weightupd[1],\n",
    "                                                l_rate_over_size=l_rate_over_size, \n",
    "                                                lambda_reg=0))\n",
    "\n",
    "print(weights1.collect())\n",
    "print(\">> \", time.time() - start, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0031901167258571073, 0.00082167436106014781, 0.014432547883848405, 0.0026106882044988427, 0.0031657644722971343, 0.0084599784637473044, 0.0071295525245273867, 0.0049993073135099035, -0.024840270207770219, -0.011647108716614663, -0.0045550610193826275, -0.0085820760464990035, -0.014687265896140458, -0.006751192939487353, -0.010706795280926438, -0.0069295294092750776, -0.0034667555074607442, 0.0041709718615863924, 0.0023122326501843261, -0.19956927494615936, -0.0048851198770965736, 0.0094256999282125026, 0.0057148930973499358, 0.01386929172408293, 0.0014479540559942578, 0.013431000540919875, 0.0047635237321291346, 0.018776055902052856, -0.015091320088695927, -0.0060081821673392943, -0.0086906621945260533, -0.0044061378320172278, -0.0014340265898175493, -0.0057626065501577509, -0.0037193108399138537, -0.0022330215924917544, -0.0054125146416484152, 0.010852673687085628, 0.0030591627397241247, 0.0088917979252002236, 0.0076866353145709091, 0.009425181311227512, -0.0092818947745077988, 0.010427908899860497, 0.0079365079365079326, 0.026294194387134338, 0.0062871500358937576, -0.023175540813234231, -0.015097352652475207, -0.0051682662882225052, -0.0087003847026326121, -0.0064705680905491839, -0.009689554213645259, -0.0092697045317289301, -0.0030520459440057427, -0.0024423760557258233, 0.0010033199505448537, 0.005936360192013458]\n",
      ">>  3.971588373184204 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "l_rate = 2\n",
    "l_rate_over_size = l_rate / train_rdd.count()\n",
    "n_features = len(train_rdd.first())\n",
    "weights2 = np.zeros(n_features)\n",
    "\n",
    "weights2_upd = []\n",
    "\n",
    "# the key of the <key, value> pairs is the index of the feature, \n",
    "#  so that we can reduce by it and sum all the updates\n",
    "weights2_upd = train_rdd \\\n",
    ".flatMap(lambda instance : [(i_feature, get_weight_upd(i_feature, weights2, instance))\n",
    "                            for i_feature in range(n_features)])\\\n",
    ".reduceByKey(lambda x,y: x+y)\\\n",
    ".map(lambda key_value_tuple: key_value_tuple[1])\\\n",
    ".collect()\n",
    "#print(weights2_upd.collect())\n",
    "\n",
    "weights2 = [ ( weights2[i] * (1 - l_rate_over_size * 0) ) - ( l_rate_over_size * weights2_upd[i] )\n",
    "            for i in range(n_features)]\n",
    "\n",
    "print(weights2)\n",
    "print(\">> \", time.time() - start, \"s\")\n",
    "#print(\"N° of elements:   \", weights2_upd.count())\n",
    "#print(\"First element:    \", weights2_upd.first())\n",
    "#print(\"Last 10 elements: \", weights2_upd.top(10))\n",
    "#print(weights2[0])\n",
    "#print(weights2[47:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (3, 2, 1)\n",
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Cost function running time:  0.19094610214233398 s\n",
      "> iteration= 0  lrate= 2  cost_func= 0.6944994357984838\n",
      "> Iteration running time:  2.352281332015991 s\n",
      "> Cost function running time:  0.14788818359375 s\n",
      "> iteration= 10  lrate= 2  cost_func= 0.6932611389669587\n",
      "> Iteration running time:  2.152902364730835 s\n",
      "> Cost function running time:  0.16089391708374023 s\n",
      "> iteration= 20  lrate= 2  cost_func= 0.690921143921324\n",
      "> Iteration running time:  2.290107488632202 s\n",
      "> Cost function running time:  0.17609715461730957 s\n",
      "> iteration= 30  lrate= 2  cost_func= 0.6983325706744552\n",
      "> Iteration running time:  2.342585563659668 s\n",
      "> Cost function running time:  0.24168944358825684 s\n",
      "> iteration= 40  lrate= 2  cost_func= 0.6974511042633942\n",
      "> Iteration running time:  2.5515129566192627 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-2413549855c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_logistic_regression_model_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_reg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-eb5324f2c01b>\u001b[0m in \u001b[0;36mtraining_logistic_regression_model_spark\u001b[0;34m(train_rdd, l_rate, iterations, lambda_reg)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# the key of the <key, value> pairs is the index of the feature,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m#  so that we can reduce by it and sum all the updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         weights = train_rdd         .flatMap(lambda instance: [(f, (weights[f], get_weight_upd(f, weights, instance))) for f in range(n_features)])        .reduceByKey(lambda x, y: (x[0], x[1] + y[1]))        .map(lambda key_weight_weightupd: update_weight(weight_weightupd=key_weight_weightupd[1],\n\u001b[0m\u001b[1;32m     68\u001b[0m                                                         \u001b[0ml_rate_over_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml_rate_over_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                                                         lambda_reg=0))\\\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CHECK IF SPARK CODE WORKS\n",
    "import time\n",
    "start = time.time()\n",
    "x = training_logistic_regression_model_spark(train_rdd, iterations=400, l_rate=2, lambda_reg=0.0)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Error: \", error)\n",
    "print(\"Coefficients: \", coeff)\n",
    "print()\n",
    "elapsed_time = end - start\n",
    "unit = \"secs\"\n",
    "if (elapsed_time % 60 >= 1):\n",
    "    elapsed_time /= 60\n",
    "    unit = \"mins\"\n",
    "print(\">> Elapsed time: \", round(elapsed_time, 2), unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  222.99143997004023\n",
      "Coefficients:  [-1.259001687967174, -1.4624220197007904, 1.697955257699117, 1.9239835638996428, 4.886979677615824, 2.7290138841251457, 9.854909761280533, 4.572386202966409, 4.21325960350969, 1.6997636302916048, 1.5733024567694436, -1.177342970394678, 0.8182427156193635, 1.8373391338709493, 3.19830670301719, 7.336386282777127, 5.7376952208512435, 3.976985648250511, 2.1892580308168594, 3.651709991649737, 4.141910947977142, 4.282802137197996, 8.874064630898523, 4.9163017404368246, -7.941188010248594, -4.698008350280225, -6.232176031031324, -2.1608809790823877, -2.047054850747421, -4.279452527031844, -1.1730497660707464, -1.518250035796162, -2.5521827833931727, -1.2385096512713256, -1.3045083318301256, 0.12625292992493437, -1.7820596384138059, -1.1782206888639717, -2.414969912638158, -0.7291739670573102, -2.6576857801003073, -4.828570688639126, -2.834506075321738, -2.0964388783059777, -4.5698445701706, -4.274278800638462, -2.2179503088138515, -1.6787851250991581, -1.9615328178992602, -0.738154219494433, -0.8314541979966096, 4.440625431875782, 7.590697532424203, 0.8312594381765971, 2.0634190440992515, 2.2690292488956945, 5.455596363108218, -1.7299129267195192]\n",
      "\n",
      ">> Elapsed time:  28.56 secs\n"
     ]
    }
   ],
   "source": [
    "# CHECK IF NORMAL CODE (NO SPARK) WORKS\n",
    "import time\n",
    "start = time.time()\n",
    "error_coeff = training_logistic_regression_model(trainingset_rdd.collect(), 10, 400, 0.0)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Error: \", error_coeff[0])\n",
    "print(\"Coefficients: \", error_coeff[1])\n",
    "print()\n",
    "print(\">> Elapsed time: \", round(end - start, 2), \"secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff = [0.18450310814753418, -0.1018614574807301, -51.80266262562908]\n",
    "predict(coeff,a.collect()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22585092248056826 0.0\n",
      "0.7752016855553817 1.0\n",
      "0.0654371540627735 0.0\n",
      "0.23733170070721787 0.0\n",
      "0.03916783792172873 0.0\n",
      "0.8937775502383307 1.0\n",
      "0.24262462014478559 1.0\n",
      "0.005817346885660601 0.0\n",
      "0.0601846158323579 0.0\n",
      "0.01960031795080852 0.0\n",
      "0.2059994377709945 0.0\n",
      "0.24197988171014795 0.0\n"
     ]
    }
   ],
   "source": [
    "data = trainingset_rdd.collect()\n",
    "# coeff = [40, 51, -153] optimum\n",
    "coeff = training_logistic_regression_model(data, 10, 4000, 0.5)[1] # Check if code works\n",
    "#coeff = [random.uniform(0, 1) for index in range(len(training_set_rdd.first()))]\n",
    "#coeff = [0.0 for index in range(len(training_set_rdd.first()))]\n",
    "for index in range(a.count()):\n",
    "    print(predict(coeff,data[index]), data[index][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0\n",
      "4.6629367034256575e-15 0\n",
      "3.88383322929009e-05 0\n",
      "0.0 0\n",
      "3.175237850427948e-14 0\n",
      "1.5401013797600172e-12 0\n",
      "0.9999999999999998 1\n",
      "0.9999999999999998 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n"
     ]
    }
   ],
   "source": [
    "coeff = [40, 51, -153]\n",
    "coeff = error_coeff[1]\n",
    "#coeff = [random.uniform(0, 1) for index in range(len(training_set_rdd.first()))]\n",
    "#coeff = [0.0 for index in range(len(training_set_rdd.first()))]\n",
    "for index in range(a.count()):     \n",
    "    print(predict(coeff,a.collect()[index]), a.collect()[index][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">iteration= 0  lrate= 5.0  error= 1.6116474406425663 cost= 3.872148020632097\n",
      "0.21599288750875967 0  -  0.26061790903180104 1\n",
      ">iteration= 1  lrate= 5.0  error= 1.7504549155878188 cost= 5.7485086700201915\n",
      "0.8080865512827858 0  -  0.9479629150397929 1\n",
      ">iteration= 2  lrate= 5.0  error= 3.9483504738736555 cost= -4.730383918255728\n",
      "0.001116196463132768 0  -  0.0008646378870037497 1\n",
      ">iteration= 3  lrate= 5.0  error= 4.85213326774374 cost= 5.201040605408044\n",
      "0.9857967152559239 0  -  0.9998046755047099 1\n",
      "0.6122069358825684\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "error_coeff = training_logistic_regression_model(a.collect(), 50, 4, 0.1) # Check if code works\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trainingset_rdd.map(lambda x: [x[1],x[2],x[3],x[-1]]).collect()\n",
    "a = sc.parallelize([\n",
    "    [0,   0,   0],\n",
    "    [0.5, 2,   0],\n",
    "    [1,   1.8, 0],\n",
    "    [0.5, 1.5, 0],\n",
    "    [1,   1.5, 0],\n",
    "    [2,   0.5, 0],\n",
    "    [0.5, 3,   1],\n",
    "    [0.5, 3,   1],\n",
    "    [2,  2,    1],\n",
    "    [3,  2,    1],\n",
    "    [3,  1,    1],\n",
    "    [3,  3,    1]    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.collect()[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_rdd = trainingset_rdd.flatMap(lambda x: parallel_step_gradient(x, coeff)) \n",
    "training_set_rdd = training_set_rdd.reduceByKey(lambda x,y:  x+y )\n",
    "training_set_rdd.count()\n",
    "coeff_update=sorted(training_set_rdd.collect())\n",
    "\n",
    "\n",
    "#training_logistic_regression_model_spark(trainingset_rdd, 0.05, 1000, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ae2b1ad095c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#values_spark = optimal_learning_rate_value_spark(trainingset_rdd, max_iterations=100, lambda_reg=0.05)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcoeffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_logistic_regression_model_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-45e50400ed4f>\u001b[0m in \u001b[0;36mtraining_logistic_regression_model_spark\u001b[0;34m(training_set_rdd, l_rate, iterations, lambda_reg)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mcoeff_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtraining_set_rdd_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_set_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#lambda x: parallel_step_gradient(x, coeff))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mcoeff_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set_rdd_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#values_spark = optimal_learning_rate_value_spark(trainingset_rdd, max_iterations=100, lambda_reg=0.05)\n",
    "coeffs = training_logistic_regression_model_spark(trainingset_rdd, 0.5, 200, 0)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 89.0 failed 1 times, most recent failure: Lost task 0.0 in stage 89.0 (TID 282, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1850, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<ipython-input-59-a86513ec794c>\", line 5, in <lambda>\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1850, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<ipython-input-59-a86513ec794c>\", line 5, in <lambda>\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-a86513ec794c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrainingset_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 89.0 failed 1 times, most recent failure: Lost task 0.0 in stage 89.0 (TID 282, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1850, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<ipython-input-59-a86513ec794c>\", line 5, in <lambda>\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1850, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<ipython-input-59-a86513ec794c>\", line 5, in <lambda>\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from operator import add \n",
    "trainingset_rdd = sc.parallelize([(1,2),(2,3),(4,5),(6,7),(8,9)])\n",
    "#trainingset_rdd = sc.parallelize([1,2,2,3,4,5,6,7,8,9])\n",
    "a= \n",
    "b = a.reduceByKey(lambda x,y: add(sum(x),sum(y)))\n",
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f91d047ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.histogram([1,2,3,4,5,6,7,8,9,2,3,4,5,2,3,4,2,3,4,5,2,6,8])\n",
    "b = [1,2,3,4,5,6,7,8,9,2,3,4,5,2,3,4,2,3,4,5,2,6,8]\n",
    "b = trainingset_rdd.map(lambda x: x[-1]).collect()\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = b\n",
    "\n",
    "# fixed bin size\n",
    "bins = np.arange(-100, 100, 5) # fixed bin size\n",
    "\n",
    "plt.hist(data, bins=11, alpha=0.5)\n",
    "plt.title('Random Gaussian data (fixed bin size)')\n",
    "plt.xlabel('variable X (bin size = 5)')\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "l_rate = 0.3\n",
    "n_epoch = 100\n",
    "\n",
    "training_logistic_regression_model(dataset, l_rate, n_epoch,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_learning_rate_value(training_set, initial_l_rate = 0.05, max_iterations = 20000, lambda_reg = 0):\n",
    "    order_log_list = [initial_l_rate]\n",
    "    [ order_log_list.append(order_log_list[-1]*(2)) for index in range(10)]\n",
    "    error_coeff_list = []\n",
    "    for l_rate in order_log_list:\n",
    "        error_coeff_list.append(training_logistic_regression_model(training_set, l_rate, max_iterations, lambda_reg))\n",
    "    return sorted(error_coeff_list)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values2 = optimal_learning_rate_value(trainingset_rdd.collect(), max_iterations=60, lambda_reg=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = optimal_learning_rate_value(trainingset_rdd.collect(), lambda_reg=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "bla, bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "bla, bla"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
