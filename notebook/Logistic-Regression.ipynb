{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Table of Contents\n",
    "1.  [Introduction](#introduction)\n",
    "2.  [Dataset](#dataset)\n",
    "     1. [Attribute Analysis](#attribute-analysis)\n",
    "3.  [Logistic Regression](#logistic-regression)\n",
    "     1. [Cost Function](#cost-function)\n",
    "     2. [Gradient Descent](#gradient-descent)\n",
    "4.  [Cross Validation Procedure](#cross-validation-procedure)\n",
    "5.  [Implementation](#implementation)\n",
    "     1. [Normalization](#normalization)\n",
    "     2. [Data set splitting](#data set splitting)\n",
    "     3. [Cost Function Implementation](#cost-function-implementation)\n",
    "     4. [Gradient Descent Implementation](#cost-function-implementation)\n",
    "6.  [Experiments](#experiments)\n",
    "    1. [Performance measures](#performance measures)\n",
    "    2. [Comparison](#benchmarks)\n",
    "          1. [Standard libraries](#standard libraries)\n",
    "          2. [Numpy version](#comparison-with-numpy)\n",
    "     \n",
    "7.  [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    " \n",
    "\n",
    "More information at [the lecture of Andre Ng](https://www.coursera.org/learn/machine-learning/lecture/QGKbr/model-selection-and-train-validation-test-sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Spambase dataset contains 4601 instances of different emails classified into spam or legit email according 57 continious values plus the nominal class which is the label. \n",
    "\n",
    "The dataset was produced by Hewlett-Packard Labs in Juky 1999.\n",
    "\n",
    "The dataset is uploaded in [this repository](ftp://ftp.ics.uci.edu/pub/machine-learning-databases/spambase/).\n",
    "\n",
    "### Attribute analysis\n",
    "\n",
    "All attributes are continuous which make suitable to use regression, in our case, logistic regression as we want to detect if there is spam(dependent variable) depending of a set of attributes(predictors or independent variables).\n",
    "\n",
    "The attribute statistics are summarized in this tabble:\n",
    "\n",
    "| Column |Min: |Max: |Average: |Std.Dev: |Coeff.Var_%: |\n",
    "|--|--|-----|--------|--------|----|\n",
    "|1 |0 |4.54 |0.10455 |0.30536 |292 |\n",
    "|2 |0 |14.28 |0.21301 |1.2906 |606 |\n",
    "|3 |0 |5.1 |0.28066 |0.50414 |180 |\n",
    "|4 |0 |42.81 |0.065425 |1.3952 |2130 |\n",
    "|5 |0 |10 |0.31222 |0.67251 |215 |\n",
    "|6 |0 |5.88 |0.095901 |0.27382 |286 |\n",
    "|7 |0 |7.27 |0.11421 |0.39144 |343 |\n",
    "|8 |0 |11.11 |0.10529 |0.40107 |381 |\n",
    "|9 |0 |5.26 |0.090067 |0.27862 |309 |\n",
    "|10 |0 |18.18 |0.23941 |0.64476 |269 |\n",
    "|11 |0 |2.61 |0.059824 |0.20154 |337 |\n",
    "|12 |0 |9.67 |0.5417 |0.8617 |159 |\n",
    "|13 |0 |5.55 |0.09393 |0.30104 |320 |\n",
    "|14 |0 |10 |0.058626 |0.33518 |572 |\n",
    "|15 |0 |4.41 |0.049205 |0.25884 |526 |\n",
    "|16 |0 |20 |0.24885 |0.82579 |332 |\n",
    "|17 |0 |7.14 |0.14259 |0.44406 |311 |\n",
    "|18 |0 |9.09 |0.18474 |0.53112 |287 |\n",
    "|19 |0 |18.75 |1.6621 |1.7755 |107 |\n",
    "|20 |0 |18.18 |0.085577 |0.50977 |596 |\n",
    "|21 |0 |11.11 |0.80976 |1.2008 |148 |\n",
    "|22 |0 |17.1 |0.1212 |1.0258 |846 |\n",
    "|23 |0 |5.45 |0.10165 |0.35029 |345 |\n",
    "|24 |0 |12.5 |0.094269 |0.44264 |470 |\n",
    "|25 |0 |20.83 |0.5495 |1.6713 |304 |\n",
    "|26 |0 |16.66 |0.26538 |0.88696 |334 |\n",
    "|27 |0 |33.33 |0.7673 |3.3673 |439 |\n",
    "|28 |0 |9.09 |0.12484 |0.53858 |431 |\n",
    "|29 |0 |14.28 |0.098915 |0.59333 |600 |\n",
    "|30 |0 |5.88 |0.10285 |0.45668 |444 |\n",
    "|31 |0 |12.5 |0.064753 |0.40339 |623 |\n",
    "|32 |0 |4.76 |0.047048 |0.32856 |698 |\n",
    "|33 |0 |18.18 |0.097229 |0.55591 |572 |\n",
    "|34 |0 |4.76 |0.047835 |0.32945 |689 |\n",
    "|35 |0 |20 |0.10541 |0.53226 |505 |\n",
    "|36 |0 |7.69 |0.097477 |0.40262 |413 |\n",
    "|37 |0 |6.89 |0.13695 |0.42345 |309 |\n",
    "|38 |0 |8.33 |0.013201 |0.22065 |1670 |\n",
    "|39 |0 |11.11 |0.078629 |0.43467 |553 |\n",
    "|40 |0 |4.76 |0.064834 |0.34992 |540 |\n",
    "|41 |0 |7.14 |0.043667 |0.3612 |827 |\n",
    "|42 |0 |14.28 |0.13234 |0.76682 |579 |\n",
    "|43 |0 |3.57 |0.046099 |0.22381 |486 |\n",
    "|44 |0 |20 |0.079196 |0.62198 |785 |\n",
    "|45 |0 |21.42 |0.30122 |1.0117 |336 |\n",
    "|46 |0 |22.05 |0.17982 |0.91112 |507 |\n",
    "|47 |0 |2.17 |0.0054445 |0.076274 |1400 |\n",
    "|48 |0 |10 |0.031869 |0.28573 |897 |\n",
    "|49 |0 |4.385 |0.038575 |0.24347 |631 |\n",
    "|50 |0 |9.752 |0.13903 |0.27036 |194 |\n",
    "|51 |0 |4.081 |0.016976 |0.10939 |644 |\n",
    "|52 |0 |32.478 |0.26907 |0.81567 |303 |\n",
    "|53 |0 |6.003 |0.075811 |0.24588 |324 |\n",
    "|54 |0 |19.829 |0.044238 |0.42934 |971 |\n",
    "|55 |1 |1102.5 |5.1915 |31.729 |611 |\n",
    "|56 |1 |9989 |52.173 |194.89 |374 |\n",
    "|57 |1 |15841 |283.29 |606.35 |214 |\n",
    "|58 |0 |1 |0.39404 |0.4887 |124 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression(LR) is an algorithm used in supervised problems where the data is labeled previously and it is needed a binary or dichotomous classifier.\n",
    "\n",
    "The LR algorithm it is based on linear regression and the logistic function...\n",
    "\n",
    "### Cost function\n",
    "\n",
    "The cost function applied in logistic regression \n",
    "\n",
    "\n",
    "$Cost(h_{\\theta}(x),y)=\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  -log({h_{\\theta}(x)})    & \\quad \\text{if } x \\text{ is 1}\\\\\n",
    "                  -log({1-h_{\\theta}(x)})  & \\quad \\text{if } n \\text{ is 0}\n",
    "                \\end{array}\n",
    "              \\right.$\n",
    "              \n",
    "Intuitively: the cost funtions returns 0 if the hypothesis function output is equal to the true label. In case the model states a diferent output the cost funtion will approach infinity.\n",
    "              \n",
    "If we reduce the formula in a more compact way we get: \n",
    "$Cost(h_{\\theta}(x),y) = -y·log(h_{\\theta}(x)) - (1-y)log(1-h_{\\theta}(x))$\n",
    "\n",
    "\n",
    "Info obtained from [lecture of Andrew NG](https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Procedure\n",
    "\n",
    "bla, bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Split of data in different datasets\n",
    "\n",
    "We will split our data in three files, for:\n",
    "- Training the model ~ 60%\n",
    "- Validation the model ~ 20%\n",
    "- Test the model ~ 20%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File spam.data has been splitted in 3 files: spam.training.data, spam.validating.data, spam.testing.data\n",
      "Sizes: \n",
      " - Training data:  2761 \n",
      " - Validating data:  920 \n",
      " - Testing data:  920 \n",
      " -- Total size; 4601\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "file_object  = open('spam.data', 'r')\n",
    "\n",
    "lines = file_object.readlines()\n",
    "file_object.close()\n",
    "    \n",
    "total_size = len(lines)\n",
    "training_size = round(total_size*0.6)\n",
    "validating_size = round(total_size*0.2)\n",
    "\n",
    "training_file  = open('spam.trainig.data', 'w')\n",
    "validating_file  = open('spam.validating.data', 'w')\n",
    "testing_file  = open('spam.testing.data', 'w')\n",
    "\n",
    "training_obj = lines[:training_size]\n",
    "validating_obj = lines[training_size:(training_size+validating_size)]\n",
    "testing_obj = lines[(training_size+validating_size):total_size]\n",
    "\n",
    "training_file.write(str(training_obj))\n",
    "validating_file.write(str(validating_obj))\n",
    "testing_file.write(str(testing_obj))\n",
    "\n",
    "training_file.close()\n",
    "validating_file.close()\n",
    "testing_file.close()\n",
    "\n",
    "print(\"File spam.data has been splitted in 3 files: spam.training.data, spam.validating.data, spam.testing.data\")\n",
    "print(\"Sizes: \\n - Training data: \", len(training_obj),\"\\n - Validating data: \",len(validating_obj), \"\\n - Testing data: \",len(testing_obj),\"\\n -- Total size:\", total_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster configuration and RDD creatioin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"Spam Filter\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Creating RDDs\n",
    "training_obj_rdd = sc.parallelize(training_obj)\n",
    "validating_obj_rdd = sc.parallelize(validating_obj)\n",
    "testing_obj_rdd = sc.parallelize(testing_obj)\n",
    "\n",
    "# Creating the list from the string\n",
    "training_obj_rdd = training_obj_rdd.map(lambda x: x.split(\" \"))\n",
    "validating_obj_rdd = validating_obj_rdd.map(lambda x: x.split(\" \"))\n",
    "testing_obj_rdd = testing_obj_rdd.map(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RDD\n",
    "master_rdd = sc.parallelize(lines)\n",
    "master_rdd = master_rdd.map(lambda x: x.split('\\n')[0].split(\" \"))\n",
    "\n",
    "# Get stats of the instance values\n",
    "max_min_rdd = master_rdd.flatMap(lambda x: [ (index_key, float(x[index_key])) for index_key in range(len(x))])\n",
    "max_list = max_min_rdd.reduceByKey(lambda x,y: x if x > y else y).collect()\n",
    "min_list = max_min_rdd.reduceByKey(lambda x,y: x if x < y else y).collect()\n",
    "mean_list = [ value[1]/len(lines) for value in max_min_rdd.reduceByKey(lambda x,y: x + y).collect()]\n",
    "variance_list = [ math.sqrt((value[1]/len(lines))) for value in max_min_rdd.map(lambda x:  [ x[0],(x[1] - mean_list[x[0]])*(x[1] - mean_list[x[0]]) ]).reduceByKey(lambda x,y: x+y).collect() ]\n",
    "coeff_variation = [ variance_list[index]/mean_list[index] for index in range(len(variance_list))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "0.002498626708984375\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "print(\"hello\")\n",
    "print(\"hello\")\n",
    "print(\"hello\")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', 'me', 'tienen', 'que', 'dividir'] hola me tienen que dividir\n"
     ]
    }
   ],
   "source": [
    "a = \"hola me tienen que dividir\"\n",
    "\n",
    "b = a.split(\" \")\n",
    "\n",
    "print(b,a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "From the [previous chapter](#), the cost function was defined as:\n",
    "\n",
    "$Cost(h_{\\theta}(x),y) = -y·log(h_{\\theta}(x)) - (1-y)log(1-h_{\\theta}(x))$\n",
    "\n",
    "J(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "bla, bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "bla, bla"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
