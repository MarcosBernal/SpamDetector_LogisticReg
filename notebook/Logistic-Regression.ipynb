{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Table of Contents\n",
    "1.  [Introduction](#introduction)\n",
    "2.  [Dataset](#dataset)\n",
    "     1. [Attribute Analysis](#attribute-analysis)\n",
    "3.  [Logistic Regression](#logistic-regression)\n",
    "     1. [Cost Function](#cost-function)\n",
    "     2. [Gradient Descent](#gradient-descent)\n",
    "4.  [Cross Validation Procedure](#cross-validation-procedure)\n",
    "5.  [Implementation](#implementation)\n",
    "     1. [Normalization](#normalization)\n",
    "     2. [Data set splitting](#data set splitting)\n",
    "     3. [Cost Function Implementation](#cost-function-implementation)\n",
    "     4. [Gradient Descent Implementation](#cost-function-implementation)\n",
    "6.  [Experiments](#experiments)\n",
    "    1. [Performance measures](#performance measures)\n",
    "    2. [Comparison](#benchmarks)\n",
    "          1. [Standard libraries](#standard libraries)\n",
    "          2. [Numpy version](#comparison-with-numpy)\n",
    "     \n",
    "7.  [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    " \n",
    "\n",
    "More information at:\n",
    "- [Machine Learning Coursera - Andre Ng](https://www.coursera.org/learn/machine-learning/)\n",
    "- [Lecture Notes pg(16-19)](http://cs229.stanford.edu/notes/cs229-notes1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Spambase dataset contains 4601 instances of different emails classified into spam or legit email according 57 continious values plus the nominal class which is the label. \n",
    "\n",
    "The dataset was produced by Hewlett-Packard Labs in Juky 1999.\n",
    "\n",
    "The dataset is uploaded in [this repository](ftp://ftp.ics.uci.edu/pub/machine-learning-databases/spambase/).\n",
    "\n",
    "### Attribute analysis\n",
    "\n",
    "All attributes are continuous which make suitable to use regression, in our case, logistic regression as we want to detect if there is spam(dependent variable) depending of a set of attributes(predictors or independent variables).\n",
    "\n",
    "The attribute statistics are summarized in this tabble:\n",
    "\n",
    "| Column |Min: |Max: |Average: |Std.Dev: |Coeff.Var_%: |\n",
    "|--|--|-----|--------|--------|----|\n",
    "|1 |0 |4.54 |0.10455 |0.30536 |292 |\n",
    "|2 |0 |14.28 |0.21301 |1.2906 |606 |\n",
    "|3 |0 |5.1 |0.28066 |0.50414 |180 |\n",
    "|4 |0 |42.81 |0.065425 |1.3952 |2130 |\n",
    "|5 |0 |10 |0.31222 |0.67251 |215 |\n",
    "|6 |0 |5.88 |0.095901 |0.27382 |286 |\n",
    "|7 |0 |7.27 |0.11421 |0.39144 |343 |\n",
    "|8 |0 |11.11 |0.10529 |0.40107 |381 |\n",
    "|9 |0 |5.26 |0.090067 |0.27862 |309 |\n",
    "|10 |0 |18.18 |0.23941 |0.64476 |269 |\n",
    "|11 |0 |2.61 |0.059824 |0.20154 |337 |\n",
    "|12 |0 |9.67 |0.5417 |0.8617 |159 |\n",
    "|13 |0 |5.55 |0.09393 |0.30104 |320 |\n",
    "|14 |0 |10 |0.058626 |0.33518 |572 |\n",
    "|15 |0 |4.41 |0.049205 |0.25884 |526 |\n",
    "|16 |0 |20 |0.24885 |0.82579 |332 |\n",
    "|17 |0 |7.14 |0.14259 |0.44406 |311 |\n",
    "|18 |0 |9.09 |0.18474 |0.53112 |287 |\n",
    "|19 |0 |18.75 |1.6621 |1.7755 |107 |\n",
    "|20 |0 |18.18 |0.085577 |0.50977 |596 |\n",
    "|21 |0 |11.11 |0.80976 |1.2008 |148 |\n",
    "|22 |0 |17.1 |0.1212 |1.0258 |846 |\n",
    "|23 |0 |5.45 |0.10165 |0.35029 |345 |\n",
    "|24 |0 |12.5 |0.094269 |0.44264 |470 |\n",
    "|25 |0 |20.83 |0.5495 |1.6713 |304 |\n",
    "|26 |0 |16.66 |0.26538 |0.88696 |334 |\n",
    "|27 |0 |33.33 |0.7673 |3.3673 |439 |\n",
    "|28 |0 |9.09 |0.12484 |0.53858 |431 |\n",
    "|29 |0 |14.28 |0.098915 |0.59333 |600 |\n",
    "|30 |0 |5.88 |0.10285 |0.45668 |444 |\n",
    "|31 |0 |12.5 |0.064753 |0.40339 |623 |\n",
    "|32 |0 |4.76 |0.047048 |0.32856 |698 |\n",
    "|33 |0 |18.18 |0.097229 |0.55591 |572 |\n",
    "|34 |0 |4.76 |0.047835 |0.32945 |689 |\n",
    "|35 |0 |20 |0.10541 |0.53226 |505 |\n",
    "|36 |0 |7.69 |0.097477 |0.40262 |413 |\n",
    "|37 |0 |6.89 |0.13695 |0.42345 |309 |\n",
    "|38 |0 |8.33 |0.013201 |0.22065 |1670 |\n",
    "|39 |0 |11.11 |0.078629 |0.43467 |553 |\n",
    "|40 |0 |4.76 |0.064834 |0.34992 |540 |\n",
    "|41 |0 |7.14 |0.043667 |0.3612 |827 |\n",
    "|42 |0 |14.28 |0.13234 |0.76682 |579 |\n",
    "|43 |0 |3.57 |0.046099 |0.22381 |486 |\n",
    "|44 |0 |20 |0.079196 |0.62198 |785 |\n",
    "|45 |0 |21.42 |0.30122 |1.0117 |336 |\n",
    "|46 |0 |22.05 |0.17982 |0.91112 |507 |\n",
    "|47 |0 |2.17 |0.0054445 |0.076274 |1400 |\n",
    "|48 |0 |10 |0.031869 |0.28573 |897 |\n",
    "|49 |0 |4.385 |0.038575 |0.24347 |631 |\n",
    "|50 |0 |9.752 |0.13903 |0.27036 |194 |\n",
    "|51 |0 |4.081 |0.016976 |0.10939 |644 |\n",
    "|52 |0 |32.478 |0.26907 |0.81567 |303 |\n",
    "|53 |0 |6.003 |0.075811 |0.24588 |324 |\n",
    "|54 |0 |19.829 |0.044238 |0.42934 |971 |\n",
    "|55 |1 |1102.5 |5.1915 |31.729 |611 |\n",
    "|56 |1 |9989 |52.173 |194.89 |374 |\n",
    "|57 |1 |15841 |283.29 |606.35 |214 |\n",
    "|58 |0 |1 |0.39404 |0.4887 |124 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression(LR) is an algorithm used in supervised problems where the data is labeled previously and it is needed a binary or dichotomous classifier.\n",
    "\n",
    "The LR algorithm it is based on linear regression and the logistic function...\n",
    "\n",
    "### Cost function\n",
    "\n",
    "The cost function applied in logistic regression \n",
    "\n",
    "\n",
    "$Cost(h_{\\theta}(x),y)=\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  -log({h_{\\theta}(x)})    & \\quad \\text{if } x \\text{ is 1}\\\\\n",
    "                  -log({1-h_{\\theta}(x)})  & \\quad \\text{if } n \\text{ is 0}\n",
    "                \\end{array}\n",
    "              \\right.$\n",
    "              \n",
    "Intuitively: the cost funtions returns 0 if the hypothesis function output is equal to the true label. In case the model states a diferent output the cost funtion will approach infinity.\n",
    "              \n",
    "If we reduce the formula in a more compact way we get: \n",
    "$Cost(h_{\\theta}(x),y) = -y·log(h_{\\theta}(x)) - (1-y)log(1-h_{\\theta}(x))$\n",
    "\n",
    "\n",
    "Info obtained from [lecture of Andrew NG](https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Procedure\n",
    "\n",
    "bla, bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster configuration and RDD creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object  = open('spam.data', 'r')\n",
    "lines = file_object.readlines()\n",
    "file_object.close()\n",
    "    \n",
    "total_size = len(lines)\n",
    "\n",
    "import math, time\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from pyspark import SparkContext, SparkConf, rdd\n",
    "conf = SparkConf().setAppName(\"Spam Filter\").setMaster(\"local[4]\").set(\"spark.hadoop.validateOutputSpecs\", \"false\");\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "Calculated stats of the features of all instances: max, min, mean, std_deviation, and coeff_variation\n",
      "Values have been normalized and are kept in the 'master_norm_rdd' variable\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Creating RDD\n",
    "master_rdd = sc.parallelize(lines)\n",
    "master_rdd = master_rdd.map(lambda x: [float(item) for item in x.split('\\n')[0].split(\" \")])\n",
    "temporal_list = master_rdd.collect()\n",
    "shuffle(temporal_list) # Labels are all together 1's and 0's\n",
    "master_rdd = sc.parallelize(temporal_list)\n",
    "\n",
    "\n",
    "# Get stats of the instance values\n",
    "max_min_rdd = master_rdd.flatMap(lambda x: [ (index_key, x[index_key]) for index_key in range(len(x))])\n",
    "max_list = [ item[1] for item in max_min_rdd.reduceByKey(lambda x,y: x if x > y else y).collect()]\n",
    "min_list = [ item[1] for item in max_min_rdd.reduceByKey(lambda x,y: x if x < y else y).collect()]\n",
    "mean_list = [ value[1]/len(lines) for value in max_min_rdd.reduceByKey(lambda x,y: x + y).collect()]\n",
    "std_deviation_list = [ math.sqrt((value[1]/len(lines))) for value in max_min_rdd.map(lambda x:  [ x[0],(x[1] - mean_list[x[0]])*(x[1] - mean_list[x[0]]) ]).reduceByKey(lambda x,y: x+y).collect() ]\n",
    "coeff_variation = [ std_deviation_list[index]/mean_list[index] for index in range(len(std_deviation_list))]\n",
    "\n",
    "# Normalize values\n",
    "def normalize_data(data):\n",
    "    max_min = data.flatMap(lambda x: [ (index_key, x[index_key]) for index_key in range(len(x)-1)]) #Last position is label\n",
    "    max__list = sorted(max_min.reduceByKey(lambda x,y: x if x > y else y).collect())\n",
    "    min__list = sorted(max_min.reduceByKey(lambda x,y: x if x < y else y).collect())\n",
    "    mean_list = sorted([ value[1]/data.count() for value in max_min.reduceByKey(lambda x,y: x + y).collect()])\n",
    "    \n",
    "    return data.map(lambda x: [(float(x[index]) - min__list[index][1])/(max__list[index][1] - min__list[index][1]) if index != len(x)-1 else x[index] for index in range(len(x))] )\n",
    "    \n",
    "\n",
    "    \n",
    "master_norm_rdd = normalize_data(master_rdd)\n",
    "#.map(lambda x: [(x[index] - min_list[index])/(max_list[index] - min_list[index]) for index in range(len(x)-1)] )\n",
    "max_min_norm_rdd = master_norm_rdd.flatMap(lambda x: [ (index_key, x[index_key]) for index_key in range(len(x))])\n",
    "\n",
    "print(\"--\")\n",
    "print(\"Calculated stats of the features of all instances: max, min, mean, std_deviation, and coeff_variation\")\n",
    "print(\"Values have been normalized and are kept in the \\'master_norm_rdd\\' variable\")\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split of data in different datasets\n",
    "\n",
    "We will split our data in three files, for:\n",
    "- Training the model ~ 60%\n",
    "- Validation the model ~ 20%\n",
    "- Test the model ~ 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File spam.data has been normalized and splitted in 3 files: \n",
      " spam.training.norm.data, spam.validating.norm.data, spam.testing.norm.data\n",
      "Sizes: \n",
      " - Training data:  2761 \n",
      " - Validating data:  920 \n",
      " - Testing data:  920 \n",
      " -- Total size: 4601\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "training_size = round(total_size*0.6)\n",
    "validating_size = round(total_size*0.2)\n",
    "\n",
    "# Creating RDDs\n",
    "master_norm_list = master_norm_rdd.collect()\n",
    "trainingset_rdd = sc.parallelize(master_norm_list[:training_size])\n",
    "validatingset_rdd = sc.parallelize(master_norm_list[training_size:training_size+validating_size])\n",
    "testingset_rdd = sc.parallelize(master_norm_list[training_size+validating_size:])\n",
    "\n",
    "trainingset_rdd.saveAsTextFile('spam.training.norm.data')\n",
    "validatingset_rdd.saveAsTextFile('spam.validating.norm.data')\n",
    "testingset_rdd.saveAsTextFile('spam.testing.norm.data')\n",
    "\n",
    "print(\"File spam.data has been normalized and splitted in 3 files: \\n spam.training.norm.data, spam.validating.norm.data, spam.testing.norm.data\")\n",
    "print(\"Sizes: \\n - Training data: \", trainingset_rdd.count() ,\"\\n - Validating data: \",validatingset_rdd.count(), \"\\n - Testing data: \",testingset_rdd.count(),\"\\n -- Total size:\", total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "values_spark = optimal_learning_rate_value_spark(trainingset_rdd, max_iterations=100, lambda_reg=0.5)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "start = time.time()\n",
    "values = optimal_learning_rate_value(trainingset_rdd.collect(), max_iterations=100, lambda_reg=0.5)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "From the [previous chapter](#), the cost function was defined as:\n",
    "\n",
    "$Cost(h_{\\theta}(x),y) = -y·log(h_{\\theta}(x)) - (1-y)log(1-h_{\\theta}(x))$\n",
    "\n",
    "J(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coeff and values must be lists with the same length\n",
    "def predict(coeff, values):\n",
    "    logit = [coeff[index]*values[index] for index in range(len(coeff)-1)] # Last element is bias(coeff) - label(values)\n",
    "    logit.append(coeff[-1])\n",
    "    logit = sum(logit)\n",
    "    \n",
    "    if logit < 0: # when logit becomes a large positive value, math.exp(gamma) overflows\n",
    "        sigmoid = 1 - 1 / (1 + math.exp(logit))\n",
    "    else:\n",
    "        sigmoid = 1 / (1 + math.exp(-logit))\n",
    "    return sigmoid\n",
    "\n",
    "\n",
    "\n",
    "# https://www.coursera.org/learn/machine-learning/lecture/10sqI/map-reduce-and-data-parallelism\n",
    "\n",
    "def training_logistic_regression_model_spark(training_set_rdd, l_rate, iterations=1000, lambda_reg = 0):\n",
    "    # Init the coeff\n",
    "    coeff = [0.0 for index in range(len(training_set_rdd.first()))]\n",
    "    l_rate_over_size = l_rate/training_set_rdd.count()\n",
    "    lambda_reg = lambda_reg/training_set_rdd.count()\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        coeff_update = []\n",
    "        training_set_rdd_mod = training_set_rdd.flatMap([(index, ((predict(x, coeff)-x[-1])*x[index])) if index is not len(x)-1 else (index, (predict(x, coeff)-x[-1]))  for index in range(len(x)) ])#lambda x: parallel_step_gradient(x, coeff)) \n",
    "        \n",
    "        coeff_update = sorted(training_set_rdd_mod.reduceByKey(lambda x,y: x+y ).collect())\n",
    "        coeff = [ coeff[index]*(1-l_rate_over_size) - (l_rate_over_size*coeff_update[index][1]) for index in range(len(coeff))]\n",
    "        \n",
    "        cost =  cost_function_spark(coeff, training_set_rdd, lambda_reg)\n",
    "        \n",
    "        if(iteration%60 == 0):\n",
    "            print('>iteration=',iteration,' lrate=',l_rate, ' cost_func=', cost)\n",
    "            \n",
    "    return ( cost, coeff )\n",
    "\n",
    "\n",
    "def parallel_step_gradient(x, coeff):\n",
    "    return [(index, ((predict(x, coeff)-x[-1])*x[index])) if index is not len(x)-1 else (index, (predict(x, coeff)-x[-1]))  for index in range(len(x)) ]\n",
    "    \n",
    "\n",
    "\n",
    "def cost_function_spark(coeff, dataset, lambda_reg=0):\n",
    "    if(not isinstance(dataset, rdd.RDD)):\n",
    "        dataset = sc.parallelize(dataset)\n",
    "\n",
    "    value = (lambda_reg/dataset.count())*sum([ coeff[index]**2 for index in range(len(coeff)-1) ]) # The bias(last elem) is not added when regula\n",
    "    value += dataset.map(lambda x: x[-1]*math.log(predict(coeff,x)) + (1-x[-1])*(1-math.log(predict(coeff,x))) ).reduce(lambda x,y: x+y)\n",
    "    \n",
    "    return value  \n",
    "\n",
    "\n",
    "def optimal_learning_rate_value_spark(training_set_rdd, initial_l_rate = 0.05, max_iterations = 20000, lambda_reg = 0):\n",
    "    order_log_list = [initial_l_rate]\n",
    "    [ order_log_list.append(order_log_list[-1]*(2)) for index in range(10)]\n",
    "    error_coeff_list = []\n",
    "    for l_rate in order_log_list:\n",
    "        error_coeff_list.append(training_logistic_regression_model_spark(training_set_rdd, l_rate, max_iterations, lambda_reg))\n",
    "    return sorted(error_coeff_list)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(228.16856095581747, [-0.4116797393583426, -0.9507346914542847, 1.2132028003351005, 1.854676910498479, 5.045134633951993, 3.443701095116759, 9.636134792751466, 4.359518865502516, 4.262926330497529, 1.42116770996463, 2.0764041118516743, -2.151325661323751, 1.330231142748374, 0.4790954801511735, 3.432546483526602, 6.578849638646296, 5.610476135573046, 3.9731737269940823, 1.5136299717242272, 3.352709175195327, 3.697101190845763, 4.431087933863464, 8.639545444390361, 3.5030077016142425, -7.780836723318914, -4.530906758408766, -7.030309329374209, -0.6060805961372036, -2.4204939893145543, -4.153931428749533, -1.2207274768537841, -1.4872731681079525, -2.365595718601463, -1.2927533798466915, -1.5198436930923926, -0.7764079981945744, -2.2856925311305556, -1.12456414277498, -2.3578284213813125, -0.736026476094526, -2.5540760260272615, -5.020149292108184, -3.0863563996583547, -2.068461947446266, -4.7784229996136816, -4.553403721489649, -0.7753308024846393, -1.4611324318925831, -2.7348062900948644, -0.7774328421670993, -0.6274260666530149, 4.534965896125456, 8.110188806790244, 1.2086047488879537, 1.6815438135275673, 2.362102332482063, 6.108599961612226, -1.5990714335009848])\n",
      "14.459816932678223\n"
     ]
    }
   ],
   "source": [
    "def training_logistic_regression_model(training_set, l_rate, iterations=1000, lambda_reg = 0):\n",
    "    # Init the coeff\n",
    "    coeff = [0.0 for index in range(len(training_set[0]))]\n",
    "    l_rate = l_rate/len(training_set)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        sum_error = 0\n",
    "        coeff_upd = [ [] for index in range(len(training_set[0]))]\n",
    "        \n",
    "        for instance in training_set:\n",
    "            yhat = predict(coeff=coeff,values=instance)\n",
    "            error = yhat - instance[-1]\n",
    "            #print('>>>pred=',yhat,\" label=\", instance[-1])\n",
    "            sum_error += error**2\n",
    "            for i in range(len(instance)-1): # Adding items of sumatorium \n",
    "                coeff_upd[i].append((error * instance[i]))\n",
    "            coeff_upd[-1].append(error) # Calculating the bias \\theta_0 \n",
    "            #print(coeff_upd)\n",
    "        #print('--',coeff)\n",
    "        \n",
    "        for i in range(len(coeff_upd)): # Regularization(1-l_rate*lambda_reg), Last value is the label (true ground), \n",
    "            coeff[i] = coeff[i]*(1-l_rate*lambda_reg) - (l_rate * sum(coeff_upd[i]))\n",
    "        \n",
    "        #cost =  cost_function_spark(coeff, training_set, lambda_reg)\n",
    "        #if(iteration%10 == 0):\n",
    "        #    print('>iteration=',iteration,' lrate=',l_rate, ' error=', sum_error, 'cost=', cost)\n",
    "        #print(predict(coeff,training_set[0]), training_set[0][-1], \" - \",predict(coeff,training_set[-1]), training_set[-1][-1])\n",
    "    return ( sum_error, coeff )\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "error_coeff = training_logistic_regression_model(trainingset_rdd.collect(), 10, 400, 0.0) # Check if code works\n",
    "print(error_coeff)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff = [0.18450310814753418, -0.1018614574807301, -51.80266262562908]\n",
    "predict(coeff,a.collect()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22585092248056826 0.0\n",
      "0.7752016855553817 1.0\n",
      "0.0654371540627735 0.0\n",
      "0.23733170070721787 0.0\n",
      "0.03916783792172873 0.0\n",
      "0.8937775502383307 1.0\n",
      "0.24262462014478559 1.0\n",
      "0.005817346885660601 0.0\n",
      "0.0601846158323579 0.0\n",
      "0.01960031795080852 0.0\n",
      "0.2059994377709945 0.0\n",
      "0.24197988171014795 0.0\n"
     ]
    }
   ],
   "source": [
    "data = trainingset_rdd.collect()\n",
    "# coeff = [40, 51, -153] optimum\n",
    "coeff = training_logistic_regression_model(data, 10, 4000, 0.5)[1] # Check if code works\n",
    "#coeff = [random.uniform(0, 1) for index in range(len(training_set_rdd.first()))]\n",
    "#coeff = [0.0 for index in range(len(training_set_rdd.first()))]\n",
    "for index in range(a.count()):\n",
    "    print(predict(coeff,data[index]), data[index][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0\n",
      "4.6629367034256575e-15 0\n",
      "3.88383322929009e-05 0\n",
      "0.0 0\n",
      "3.175237850427948e-14 0\n",
      "1.5401013797600172e-12 0\n",
      "0.9999999999999998 1\n",
      "0.9999999999999998 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n"
     ]
    }
   ],
   "source": [
    "coeff = [40, 51, -153]\n",
    "coeff = error_coeff[1]\n",
    "#coeff = [random.uniform(0, 1) for index in range(len(training_set_rdd.first()))]\n",
    "#coeff = [0.0 for index in range(len(training_set_rdd.first()))]\n",
    "for index in range(a.count()):     \n",
    "    print(predict(coeff,a.collect()[index]), a.collect()[index][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">iteration= 0  lrate= 5.0  error= 1.6116474406425663 cost= 3.872148020632097\n",
      "0.21599288750875967 0  -  0.26061790903180104 1\n",
      ">iteration= 1  lrate= 5.0  error= 1.7504549155878188 cost= 5.7485086700201915\n",
      "0.8080865512827858 0  -  0.9479629150397929 1\n",
      ">iteration= 2  lrate= 5.0  error= 3.9483504738736555 cost= -4.730383918255728\n",
      "0.001116196463132768 0  -  0.0008646378870037497 1\n",
      ">iteration= 3  lrate= 5.0  error= 4.85213326774374 cost= 5.201040605408044\n",
      "0.9857967152559239 0  -  0.9998046755047099 1\n",
      "0.6122069358825684\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "error_coeff = training_logistic_regression_model(a.collect(), 50, 4, 0.1) # Check if code works\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trainingset_rdd.map(lambda x: [x[1],x[2],x[3],x[-1]]).collect()\n",
    "a = sc.parallelize([\n",
    "    [0,   0,   0],\n",
    "    [0.5, 2,   0],\n",
    "    [1,   1.8, 0],\n",
    "    [0.5, 1.5, 0],\n",
    "    [1,   1.5, 0],\n",
    "    [2,   0.5, 0],\n",
    "    [0.5, 3,   1],\n",
    "    [0.5, 3,   1],\n",
    "    [2,  2,    1],\n",
    "    [3,  2,    1],\n",
    "    [3,  1,    1],\n",
    "    [3,  3,    1]    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.collect()[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_rdd = trainingset_rdd.flatMap(lambda x: parallel_step_gradient(x, coeff)) \n",
    "training_set_rdd = training_set_rdd.reduceByKey(lambda x,y:  x+y )\n",
    "training_set_rdd.count()\n",
    "coeff_update=sorted(training_set_rdd.collect())\n",
    "\n",
    "\n",
    "#training_logistic_regression_model_spark(trainingset_rdd, 0.05, 1000, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ae2b1ad095c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#values_spark = optimal_learning_rate_value_spark(trainingset_rdd, max_iterations=100, lambda_reg=0.05)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcoeffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_logistic_regression_model_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-45e50400ed4f>\u001b[0m in \u001b[0;36mtraining_logistic_regression_model_spark\u001b[0;34m(training_set_rdd, l_rate, iterations, lambda_reg)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mcoeff_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtraining_set_rdd_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_set_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#lambda x: parallel_step_gradient(x, coeff))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mcoeff_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set_rdd_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#values_spark = optimal_learning_rate_value_spark(trainingset_rdd, max_iterations=100, lambda_reg=0.05)\n",
    "coeffs = training_logistic_regression_model_spark(trainingset_rdd, 0.5, 200, 0)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 89.0 failed 1 times, most recent failure: Lost task 0.0 in stage 89.0 (TID 282, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1850, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<ipython-input-59-a86513ec794c>\", line 5, in <lambda>\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1850, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<ipython-input-59-a86513ec794c>\", line 5, in <lambda>\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-a86513ec794c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrainingset_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 89.0 failed 1 times, most recent failure: Lost task 0.0 in stage 89.0 (TID 282, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1850, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<ipython-input-59-a86513ec794c>\", line 5, in <lambda>\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1850, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<ipython-input-59-a86513ec794c>\", line 5, in <lambda>\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from operator import add \n",
    "trainingset_rdd = sc.parallelize([(1,2),(2,3),(4,5),(6,7),(8,9)])\n",
    "#trainingset_rdd = sc.parallelize([1,2,2,3,4,5,6,7,8,9])\n",
    "a= \n",
    "b = a.reduceByKey(lambda x,y: add(sum(x),sum(y)))\n",
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f91d047ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.histogram([1,2,3,4,5,6,7,8,9,2,3,4,5,2,3,4,2,3,4,5,2,6,8])\n",
    "b = [1,2,3,4,5,6,7,8,9,2,3,4,5,2,3,4,2,3,4,5,2,6,8]\n",
    "b = trainingset_rdd.map(lambda x: x[-1]).collect()\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = b\n",
    "\n",
    "# fixed bin size\n",
    "bins = np.arange(-100, 100, 5) # fixed bin size\n",
    "\n",
    "plt.hist(data, bins=11, alpha=0.5)\n",
    "plt.title('Random Gaussian data (fixed bin size)')\n",
    "plt.xlabel('variable X (bin size = 5)')\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "l_rate = 0.3\n",
    "n_epoch = 100\n",
    "\n",
    "training_logistic_regression_model(dataset, l_rate, n_epoch,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_learning_rate_value(training_set, initial_l_rate = 0.05, max_iterations = 20000, lambda_reg = 0):\n",
    "    order_log_list = [initial_l_rate]\n",
    "    [ order_log_list.append(order_log_list[-1]*(2)) for index in range(10)]\n",
    "    error_coeff_list = []\n",
    "    for l_rate in order_log_list:\n",
    "        error_coeff_list.append(training_logistic_regression_model(training_set, l_rate, max_iterations, lambda_reg))\n",
    "    return sorted(error_coeff_list)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values2 = optimal_learning_rate_value(trainingset_rdd.collect(), max_iterations=60, lambda_reg=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = optimal_learning_rate_value(trainingset_rdd.collect(), lambda_reg=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "bla, bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "bla, bla"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
