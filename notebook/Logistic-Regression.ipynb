{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Table of Contents\n",
    "1.  [Introduction](#introduction)\n",
    "2.  [Dataset](#dataset)\n",
    "     1. [Attribute Analysis](#attribute-analysis)\n",
    "3.  [Logistic Regression](#logistic-regression)\n",
    "     1. [Cost Function](#cost-function)\n",
    "     2. [Gradient Descent](#gradient-descent)\n",
    "4.  [Cross Validation Procedure](#cross-validation-procedure)\n",
    "5.  [Implementation](#implementation)\n",
    "     1. [Normalization](#normalization)\n",
    "     2. [Data set splitting](#data set splitting)\n",
    "     3. [Cost Function Implementation](#cost-function-implementation)\n",
    "     4. [Gradient Descent Implementation](#cost-function-implementation)\n",
    "6.  [Experiments](#experiments)\n",
    "    1. [Performance measures](#performance measures)\n",
    "    2. [Comparison](#benchmarks)\n",
    "          1. [Standard libraries](#standard libraries)\n",
    "          2. [Numpy version](#comparison-with-numpy)\n",
    "     \n",
    "7.  [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    " \n",
    "\n",
    "More information at:\n",
    "- [Machine Learning Coursera - Andre Ng](https://www.coursera.org/learn/machine-learning/)\n",
    "- [Lecture Notes pg(16-19)](http://cs229.stanford.edu/notes/cs229-notes1.pdf)\n",
    "- SUMMARIZED version: [All course](http://www.holehouse.org/mlclass/), [Logistic Regression](http://www.holehouse.org/mlclass/06_Logistic_Regression.html) and [Regularization](http://www.holehouse.org/mlclass/07_Regularization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Spambase dataset contains 4601 instances of different emails classified into spam or legit email according 57 continious values plus the nominal class which is the label. \n",
    "\n",
    "The dataset was produced by Hewlett-Packard Labs in Juky 1999.\n",
    "\n",
    "The dataset is uploaded in [this repository](ftp://ftp.ics.uci.edu/pub/machine-learning-databases/spambase/).\n",
    "\n",
    "### Attribute analysis\n",
    "\n",
    "All attributes are continuous which make suitable to use regression, in our case, logistic regression as we want to detect if there is spam(dependent variable) depending of a set of attributes(predictors or independent variables).\n",
    "\n",
    "The attribute statistics are summarized in this tabble:\n",
    "\n",
    "| Column |Min: |Max: |Average: |Std.Dev: |Coeff.Var_%: |\n",
    "|--|--|-----|--------|--------|----|\n",
    "|1 |0 |4.54 |0.10455 |0.30536 |292 |\n",
    "|2 |0 |14.28 |0.21301 |1.2906 |606 |\n",
    "|3 |0 |5.1 |0.28066 |0.50414 |180 |\n",
    "|4 |0 |42.81 |0.065425 |1.3952 |2130 |\n",
    "|5 |0 |10 |0.31222 |0.67251 |215 |\n",
    "|6 |0 |5.88 |0.095901 |0.27382 |286 |\n",
    "|7 |0 |7.27 |0.11421 |0.39144 |343 |\n",
    "|8 |0 |11.11 |0.10529 |0.40107 |381 |\n",
    "|9 |0 |5.26 |0.090067 |0.27862 |309 |\n",
    "|10 |0 |18.18 |0.23941 |0.64476 |269 |\n",
    "|11 |0 |2.61 |0.059824 |0.20154 |337 |\n",
    "|12 |0 |9.67 |0.5417 |0.8617 |159 |\n",
    "|13 |0 |5.55 |0.09393 |0.30104 |320 |\n",
    "|14 |0 |10 |0.058626 |0.33518 |572 |\n",
    "|15 |0 |4.41 |0.049205 |0.25884 |526 |\n",
    "|16 |0 |20 |0.24885 |0.82579 |332 |\n",
    "|17 |0 |7.14 |0.14259 |0.44406 |311 |\n",
    "|18 |0 |9.09 |0.18474 |0.53112 |287 |\n",
    "|19 |0 |18.75 |1.6621 |1.7755 |107 |\n",
    "|20 |0 |18.18 |0.085577 |0.50977 |596 |\n",
    "|21 |0 |11.11 |0.80976 |1.2008 |148 |\n",
    "|22 |0 |17.1 |0.1212 |1.0258 |846 |\n",
    "|23 |0 |5.45 |0.10165 |0.35029 |345 |\n",
    "|24 |0 |12.5 |0.094269 |0.44264 |470 |\n",
    "|25 |0 |20.83 |0.5495 |1.6713 |304 |\n",
    "|26 |0 |16.66 |0.26538 |0.88696 |334 |\n",
    "|27 |0 |33.33 |0.7673 |3.3673 |439 |\n",
    "|28 |0 |9.09 |0.12484 |0.53858 |431 |\n",
    "|29 |0 |14.28 |0.098915 |0.59333 |600 |\n",
    "|30 |0 |5.88 |0.10285 |0.45668 |444 |\n",
    "|31 |0 |12.5 |0.064753 |0.40339 |623 |\n",
    "|32 |0 |4.76 |0.047048 |0.32856 |698 |\n",
    "|33 |0 |18.18 |0.097229 |0.55591 |572 |\n",
    "|34 |0 |4.76 |0.047835 |0.32945 |689 |\n",
    "|35 |0 |20 |0.10541 |0.53226 |505 |\n",
    "|36 |0 |7.69 |0.097477 |0.40262 |413 |\n",
    "|37 |0 |6.89 |0.13695 |0.42345 |309 |\n",
    "|38 |0 |8.33 |0.013201 |0.22065 |1670 |\n",
    "|39 |0 |11.11 |0.078629 |0.43467 |553 |\n",
    "|40 |0 |4.76 |0.064834 |0.34992 |540 |\n",
    "|41 |0 |7.14 |0.043667 |0.3612 |827 |\n",
    "|42 |0 |14.28 |0.13234 |0.76682 |579 |\n",
    "|43 |0 |3.57 |0.046099 |0.22381 |486 |\n",
    "|44 |0 |20 |0.079196 |0.62198 |785 |\n",
    "|45 |0 |21.42 |0.30122 |1.0117 |336 |\n",
    "|46 |0 |22.05 |0.17982 |0.91112 |507 |\n",
    "|47 |0 |2.17 |0.0054445 |0.076274 |1400 |\n",
    "|48 |0 |10 |0.031869 |0.28573 |897 |\n",
    "|49 |0 |4.385 |0.038575 |0.24347 |631 |\n",
    "|50 |0 |9.752 |0.13903 |0.27036 |194 |\n",
    "|51 |0 |4.081 |0.016976 |0.10939 |644 |\n",
    "|52 |0 |32.478 |0.26907 |0.81567 |303 |\n",
    "|53 |0 |6.003 |0.075811 |0.24588 |324 |\n",
    "|54 |0 |19.829 |0.044238 |0.42934 |971 |\n",
    "|55 |1 |1102.5 |5.1915 |31.729 |611 |\n",
    "|56 |1 |9989 |52.173 |194.89 |374 |\n",
    "|57 |1 |15841 |283.29 |606.35 |214 |\n",
    "|58 |0 |1 |0.39404 |0.4887 |124 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression(LR) is an algorithm used in supervised problems where the data is labeled previously and it is needed a binary or dichotomous classifier.\n",
    "\n",
    "The LR algorithm it is based on linear regression and the logistic function...\n",
    "\n",
    "### Cost function\n",
    "\n",
    "The cost function applied in logistic regression \n",
    "\n",
    "\n",
    "$Cost(h_{\\theta}(x),y)=\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  -log({h_{\\theta}(x)})    & \\quad \\text{if } x \\text{ is 1}\\\\\n",
    "                  -log({1-h_{\\theta}(x)})  & \\quad \\text{if } n \\text{ is 0}\n",
    "                \\end{array}\n",
    "              \\right.$\n",
    "              \n",
    "Intuitively: the cost funtions returns 0 if the hypothesis function output is equal to the true label. In case the model states a diferent output the cost funtion will approach infinity.\n",
    "              \n",
    "If we reduce the formula in a more compact way we get: \n",
    "$Cost(h_{\\theta}(x),y) = -y·log(h_{\\theta}(x)) - (1-y)log(1-h_{\\theta}(x))$\n",
    "\n",
    "\n",
    "Info obtained from [lecture of Andrew NG](https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Procedure\n",
    "\n",
    "bla, bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster configuration and RDD creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object  = open('spam.data', 'r')\n",
    "lines = file_object.readlines()\n",
    "file_object.close()\n",
    "    \n",
    "total_size = len(lines)\n",
    "\n",
    "import math, time, random\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from pyspark import SparkContext, SparkConf, rdd\n",
    "conf = SparkConf().setAppName(\"Spam Filter\").setMaster(\"local[1]\").set(\"spark.hadoop.validateOutputSpecs\", \"false\");\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RDD\n",
    "master_rdd = sc.parallelize(lines)\n",
    "master_rdd = master_rdd.map(lambda x: [float(item) for item in x.split('\\n')[0].split(\" \")])\n",
    "temporal_list = master_rdd.collect()\n",
    "shuffle(temporal_list) # Labels are all together 1's and 0's\n",
    "master_rdd = sc.parallelize(temporal_list)\n",
    "\n",
    "\n",
    "# Get stats of the instance values\n",
    "max_min_rdd = master_rdd.flatMap(lambda x: [ (index_key, x[index_key]) for index_key in range(len(x))])\n",
    "max_list = [ item[1] for item in max_min_rdd.reduceByKey(lambda x,y: x if x > y else y).collect()]\n",
    "min_list = [ item[1] for item in max_min_rdd.reduceByKey(lambda x,y: x if x < y else y).collect()]\n",
    "mean_list = [ value[1]/len(lines) for value in max_min_rdd.reduceByKey(lambda x,y: x + y).collect()]\n",
    "std_deviation_list = [ math.sqrt((value[1]/len(lines))) for value in max_min_rdd.map(lambda x:  [ x[0],(x[1] - mean_list[x[0]])*(x[1] - mean_list[x[0]]) ]).reduceByKey(lambda x,y: x+y).collect() ]\n",
    "coeff_variation = [ std_deviation_list[index]/mean_list[index] for index in range(len(std_deviation_list))]\n",
    "\n",
    "# Normalize values\n",
    "def normalize_data(data):\n",
    "    max_min = data.flatMap(lambda x: [ (index_key, x[index_key]) for index_key in range(len(x)-1)]) #Last position is label\n",
    "    max__list = sorted(max_min.reduceByKey(lambda x,y: x if x > y else y).collect())\n",
    "    min__list = sorted(max_min.reduceByKey(lambda x,y: x if x < y else y).collect())\n",
    "    mean_list = sorted([ value[1]/data.count() for value in max_min.reduceByKey(lambda x,y: x + y).collect()])\n",
    "    \n",
    "    return data.map(lambda x: [(float(x[index]) - min__list[index][1])/(max__list[index][1] - min__list[index][1]) if index != len(x)-1 else x[index] for index in range(len(x))] )\n",
    "    \n",
    "\n",
    "    \n",
    "master_norm_rdd = normalize_data(master_rdd)\n",
    "#.map(lambda x: [(x[index] - min_list[index])/(max_list[index] - min_list[index]) for index in range(len(x)-1)] )\n",
    "max_min_norm_rdd = master_norm_rdd.flatMap(lambda x: [ (index_key, x[index_key]) for index_key in range(len(x))])\n",
    "\n",
    "print(\"--\")\n",
    "print(\"Calculated stats of the features of all instances: max, min, mean, std_deviation, and coeff_variation\")\n",
    "print(\"Values have been normalized and are kept in the \\'master_norm_rdd\\' variable\")\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split of data in different datasets\n",
    "\n",
    "We will split our data in three files, for:\n",
    "- Training the model ~ 60%\n",
    "- Validation the model ~ 20%\n",
    "- Test the model ~ 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "training_size = round(total_size*0.6)\n",
    "validating_size = round(total_size*0.2)\n",
    "\n",
    "# Creating RDDs\n",
    "master_norm_list = master_norm_rdd.collect()\n",
    "trainingset_rdd = sc.parallelize(master_norm_list[:training_size])\n",
    "validatingset_rdd = sc.parallelize(master_norm_list[training_size:training_size+validating_size])\n",
    "testingset_rdd = sc.parallelize(master_norm_list[training_size+validating_size:])\n",
    "\n",
    "trainingset_rdd.saveAsTextFile('spam.training.norm.data')\n",
    "validatingset_rdd.saveAsTextFile('spam.validating.norm.data')\n",
    "testingset_rdd.saveAsTextFile('spam.testing.norm.data')\n",
    "\n",
    "print(\"File \\'spam.data\\' has been normalized and splitted in 3 files: \", \n",
    "      \"\\n \\'spam.training.norm.data\\', \\'spam.validating.norm.data\\', \\'spam.testing.norm.data\\'\")\n",
    "print(\"Sizes: \",\n",
    "      \"\\n - Training data: \", trainingset_rdd.count() , \" samples\",\n",
    "      \"\\n - Validating data: \",validatingset_rdd.count(), \" s.\",\n",
    "      \"\\n - Testing data: \",testingset_rdd.count(), \" s.\",\n",
    "      \"\\n -- Total size:\", total_size, \" s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "From the [previous chapter](#), the cost function was defined as:\n",
    "\n",
    "$Cost(h_{\\theta}(x),y) = -y·log(h_{\\theta}(x)) - (1-y)log(1-h_{\\theta}(x))$\n",
    "\n",
    "where $h_{\\theta}(x)$ is the predicted label and $y$ the true label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coeff and values must be lists with the same length\n",
    "def predict(coeff, values):\n",
    "    logit = [ coeff[index] * values[index] if (index < len(coeff) -1)\n",
    "             else coeff[-1] # Last element is bias(coeff) - label(values)\n",
    "             for index in range(len(coeff)) ] \n",
    "    logit = sum(logit)\n",
    "    \n",
    "    if logit < 0: # when logit becomes a large positive value, math.exp(gamma) overflows\n",
    "        sigmoid = 1 - 1 / (1 + math.exp(logit))\n",
    "    else:\n",
    "        sigmoid = 1 / (1 + math.exp(-logit))\n",
    "    return sigmoid\n",
    "\n",
    "\n",
    "def training_logistic_regression_model(training_set, l_rate, iterations=1000, lambda_reg = 0, show_iteration=0):\n",
    "    \n",
    "    if show_iteration > iterations:\n",
    "        show_iteration = iterations-1\n",
    "    \n",
    "    # Init the coeff\n",
    "    n_features = len(training_set[0])\n",
    "    \n",
    "    weights = [0.0 for index in range(n_features)]\n",
    "    l_rate_over_m = l_rate/len(training_set)\n",
    "    \n",
    "    # See http://holehouse.org/mlclass/07_Regularization.html for details\n",
    "    for iteration in range(iterations):\n",
    "        sum_error = 0\n",
    "        weights_upd = [ [] for index in range(n_features)]\n",
    "        \n",
    "        for instance in training_set:\n",
    "            y = instance[-1]   # the last element of each row is the true label\n",
    "            yhat = predict(coeff=weights, \n",
    "                           values=instance)  # compute the predicted label according to current weights\n",
    "            error = yhat - y\n",
    "            \n",
    "            sum_error += error**2\n",
    "            \n",
    "            for i in range(len(instance)): # Adding items of sumatorium \n",
    "                if (i < len(instance) -1):\n",
    "                    weights_upd[i].append((error * instance[i]))\n",
    "                else:\n",
    "                    weights_upd[-1].append(error) # Calculating the bias \\theta_0 \n",
    "        \n",
    "        # Regularization\n",
    "        for i in range(n_features):  \n",
    "            weights[i] = weights[i] * (1 - l_rate_over_m * lambda_reg) - (l_rate_over_m * sum(weights_upd[i]))\n",
    "            \n",
    "        if(show_iteration > 0 and iteration%show_iteration == 0):\n",
    "            cost =  cost_function_spark(weights, training_set, lambda_reg)\n",
    "            print('>iteration=',iteration,' lrate=',l_rate, ' error=', sum_error, 'cost=', cost)\n",
    "        #print(predict(coeff,training_set[0]), training_set[0][-1], \" - \",predict(coeff,training_set[-1]), training_set[-1][-1])\n",
    "\n",
    "        \n",
    "    return ( sum_error, weights )\n",
    "\n",
    "## SPARK IMPLEMENTATIONS ###############################################################################################\n",
    "\n",
    "# https://www.coursera.org/learn/machine-learning/lecture/10sqI/map-reduce-and-data-parallelism\n",
    "\n",
    "def training_logistic_regression_model_spark(training_set_rdd, l_rate, iterations=1000, lambda_reg = 0, show_iteration=0):\n",
    "\n",
    "    # initialize the weights vector (one weight per feature)\n",
    "    n_features = len(training_set_rdd.first())\n",
    "    n_weights = n_features\n",
    "    weights = [0.0 for index in range(n_features)]\n",
    "    \n",
    "    # compute useful constants for further computations\n",
    "    l_rate_over_size = l_rate / training_set_rdd.count()\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        weights_upd = []\n",
    "        \n",
    "        # the key of the <key, value> pairs is the index of the feature, so that we can reduce by it and sum all the updates\n",
    "        training_set_rdd_mod = training_set_rdd.flatMap(\n",
    "            lambda instance : [(i_feature, get_weight_upd(i_feature, weights, instance)) \n",
    "                               for i_feature in range(n_features)]) \n",
    "        \n",
    "        weights_upd = sorted(training_set_rdd_mod.reduceByKey(lambda x,y: x+y ).collect())\n",
    "        weights_upd_values = [weights_upd[i_feature][1] for i_feature in range(n_features)]\n",
    "        \n",
    "        weights = [  ( weights[i] * (1 - l_rate_over_size * lambda_reg) ) - ( l_rate_over_size * weights_upd_values[i] ) \n",
    "                 for i in range(n_weights)]\n",
    "        \n",
    "        cost =  cost_function_spark(weights, training_set_rdd, lambda_reg)\n",
    "        \n",
    "        if(show_iteration > 0 and iteration%show_iteration == 0):\n",
    "            cost =  cost_function_spark(weights, training_set_rdd, lambda_reg)\n",
    "            print('>iteration=',iteration,' lrate=',l_rate, ' cost_func=', cost)\n",
    "            \n",
    "    return ( cost, weights )\n",
    "\n",
    "def get_weight_upd(i_feature, weights, instance):\n",
    "    n_features = len(instance)\n",
    "    if (i_feature < n_features -1):\n",
    "        return (predict(weights, instance) - instance[-1]) * instance[i_feature]\n",
    "    else:\n",
    "        return (predict(weights, instance) - instance[-1])\n",
    "\n",
    "\n",
    "def cost_function_spark(coeff, dataset, lambda_reg=0):\n",
    "    if(not isinstance(dataset, rdd.RDD)):\n",
    "        dataset = sc.parallelize(dataset)\n",
    "\n",
    "    value = (lambda_reg/2)*sum([ coeff[index]**2 for index in range(len(coeff)-1) ]) # The bias(last elem) is not added when regula\n",
    "    \n",
    "    value -= dataset.map(\n",
    "        lambda x:   x[-1] * math.log(predict(coeff, x)) \n",
    "                  + (1 - x[-1]) * (1 - math.log(predict(coeff, x)))) \\\n",
    "        .reduce(lambda x,y: x+y)\n",
    "    \n",
    "    return value/dataset.count()  \n",
    "\n",
    "\n",
    "def optimal_learning_rate_value(training_set, initial_l_rate = 0.05, list_size = 10, max_iterations = 20000, lambda_reg = 0):\n",
    "    order_log_list = [initial_l_rate]\n",
    "    [ order_log_list.append(order_log_list[-1]*(2)) for index in range(list_size)]\n",
    "    weight_list = []\n",
    "    for l_rate in order_log_list:\n",
    "        weight_list.append(training_logistic_regression_model(training_set, l_rate, max_iterations, lambda_reg, show_iteration=max_iterations/4))\n",
    "    return sorted(weight_list) \n",
    "\n",
    "def optimal_learning_rate_value_spark(training_set_rdd, initial_l_rate = 0.05, list_size = 10, max_iterations = 20000, lambda_reg = 0):\n",
    "    order_log_list = [initial_l_rate]\n",
    "    [ order_log_list.append(order_log_list[-1]*(2)) for index in range(list_size)]\n",
    "    weight_list = []\n",
    "    for l_rate in order_log_list:\n",
    "        weight_list.append(training_logistic_regression_model_spark(training_set_rdd, l_rate, max_iterations, lambda_reg, show_iteration=max_iterations/4))\n",
    "    return sorted(weight_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing values\n",
    "\n",
    "data = trainingset_rdd;\n",
    "\n",
    "start = time.time()\n",
    "trained_coeff = training_logistic_regression_model(training_set=data.collect(), iterations=121, l_rate=10, lambda_reg=0.0, show_iteration=60) # Check if code works\n",
    "#print(error_coeff)\n",
    "end = time.time()\n",
    "print(\"Computed in:\", end - start, \"seconds\")\n",
    "\n",
    "# Showing comparison with random weight and 0 vector weight\n",
    "print(\">>Showing comparison with random weight and 0 vector weight\")\n",
    "data = data.collect()[10:20]\n",
    "coeff = trained_coeff[1]#training_logistic_regression_model(data, 10, 4000, 0.5)[1] # Check if code works\n",
    "error_coeff = [random.uniform(0, 1) for index in range(len(data[0]))]\n",
    "zero_coeff = [0.0 for index in range(len(data[0]))]\n",
    "\n",
    "for index in range(len(data)):\n",
    "    print(\"Estim: \", predict(coeff,data[index]), \" Label: <\", data[index][-1],\">\", \n",
    "          \" With wrong weights: 'Random(0,1)':\",  predict(error_coeff,data[index]), \" 'Zero':\", predict(zero_coeff,data[index])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "values_spark = optimal_learning_rate_value_spark(trainingset_rdd, initial_l_rate=1, list_size=2, max_iterations=100, lambda_reg=0.5)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "start = time.time()\n",
    "values = optimal_learning_rate_value(trainingset_rdd.collect(), initial_l_rate=1, list_size=2, max_iterations=100, lambda_reg=0.0)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing sequential and spark version of logistic regression with a linear solvable problem\n",
    "\n",
    "data = sc.parallelize([\n",
    "    [0,   0,   0],\n",
    "    [0.5, 2,   0],\n",
    "    [1,   1.8, 0],\n",
    "    [0.5, 1.5, 0],\n",
    "    [1,   1.5, 0],\n",
    "    [2,   0.5, 0],\n",
    "    [0.5, 3,   1],\n",
    "    [0.5, 3,   1],\n",
    "    [2,  2,    1],\n",
    "    [3,  2,    1],\n",
    "    [3,  1,    1],\n",
    "    [3,  3,    1]    \n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "estimated_weight = training_logistic_regression_model(data.collect(), 0.5, 500, 0.01, show_iteration=100) # Check if code works\n",
    "end = time.time()\n",
    "print(\"Optimal learning rate computed sequentially.\")\n",
    "print(\" > Elapsed Time: \", end - start)\n",
    "\n",
    "estimated_weight = estimated_weight[1]\n",
    "#estimated_weight = [40, 51, -153] # Optimal\n",
    "for index in range(data.count()):     \n",
    "    print(predict(estimated_weight,a.collect()[index]), data.collect()[index][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.histogram([1,2,3,4,5,6,7,8,9,2,3,4,5,2,3,4,2,3,4,5,2,6,8])\n",
    "b = [1,2,3,4,5,6,7,8,9,2,3,4,5,2,3,4,2,3,4,5,2,6,8]\n",
    "b = trainingset_rdd.map(lambda x: x[-1]).collect()\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = b\n",
    "\n",
    "# fixed bin size\n",
    "bins = np.arange(-100, 100, 5) # fixed bin size\n",
    "\n",
    "plt.hist(data, bins=11, alpha=0.5)\n",
    "plt.title('Random Gaussian data (fixed bin size)')\n",
    "plt.xlabel('variable X (bin size = 5)')\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "bla, bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "bla, bla"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
